{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br><center><img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/27923/logos/header.png?t=2021-06-02-20-30-25\" width=100%></center>\n",
    "\n",
    "<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">UWM - GI Tract Image Segmentation Challenge - EDA</h2>\n",
    "<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: DARIEN SCHETTLER</h5>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n",
    "    <b style=\"font-size: 18px;\">üõë &nbsp; WARNING:</b><br><br><b>THIS IS A WORK IN PROGRESS</b><br>\n",
    "</div></center>\n",
    "\n",
    "\n",
    "<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n",
    "    <b style=\"font-size: 18px;\">üëè &nbsp; IF YOU FORK THIS OR FIND THIS HELPFUL &nbsp; üëè</b><br><br><b style=\"font-size: 22px; color: darkorange\">PLEASE UPVOTE!</b><br><br>This was a lot of work for me and while it may seem silly, it makes me feel appreciated when others like my work. üòÖ\n",
    "</div></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p id=\"toc\"></p>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\">TABLE OF CONTENTS</h1>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#imports\">0&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS</a></h3>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#background_information\">1&nbsp;&nbsp;&nbsp;&nbsp;BACKGROUND INFORMATION</a></h3>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#setup\">2&nbsp;&nbsp;&nbsp;&nbsp;SETUP</a></h3>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#helper_functions\">3&nbsp;&nbsp;&nbsp;&nbsp;HELPER FUNCTIONS</a></h3>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#create_dataset\">4&nbsp;&nbsp;&nbsp;&nbsp;DATASET EXPLORATION</a></h3>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#modelling\">5&nbsp;&nbsp;&nbsp;&nbsp;MODELLING</a></h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a id=\"imports\"></a>\n",
    "\n",
    "<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: teal;\" id=\"imports\">0&nbsp;&nbsp;IMPORTS&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n... IMPORTS STARTING ...\\n\")\n",
    "\n",
    "print(\"\\n\\tVERSION INFORMATION\")\n",
    "# Machine Learning and Data Science Imports\n",
    "import tensorflow as tf; print(f\"\\t\\t‚Äì TENSORFLOW VERSION: {tf.__version__}\");\n",
    "import tensorflow_hub as tfhub; print(f\"\\t\\t‚Äì TENSORFLOW HUB VERSION: {tfhub.__version__}\");\n",
    "import tensorflow_addons as tfa; print(f\"\\t\\t‚Äì TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\n",
    "import pandas as pd; pd.options.mode.chained_assignment = None;\n",
    "import numpy as np; print(f\"\\t\\t‚Äì NUMPY VERSION: {np.__version__}\");\n",
    "import sklearn; print(f\"\\t\\t‚Äì SKLEARN VERSION: {sklearn.__version__}\");\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures\n",
    "from pandarallel import pandarallel; pandarallel.initialize();\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# # RAPIDS\n",
    "# import cudf, cupy, cuml\n",
    "# from cuml.neighbors import NearestNeighbors\n",
    "# from cuml.manifold import TSNE, UMAP\n",
    "\n",
    "# Built In Imports\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import warnings\n",
    "import requests\n",
    "import hashlib\n",
    "import imageio\n",
    "import IPython\n",
    "import sklearn\n",
    "import urllib\n",
    "import zipfile\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import string\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import gzip\n",
    "import ast\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "\n",
    "# Visualization Imports\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as patches\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm; tqdm.pandas();\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageEnhance\n",
    "import matplotlib; print(f\"\\t\\t‚Äì MATPLOTLIB VERSION: {matplotlib.__version__}\");\n",
    "from matplotlib import animation, rc; rc('animation', html='jshtml')\n",
    "import plotly\n",
    "import PIL\n",
    "import cv2\n",
    "\n",
    "import plotly.io as pio\n",
    "print(pio.renderers)\n",
    "\n",
    "def seed_it_all(seed=7):\n",
    "    \"\"\" Attempt to be Reproducible \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    \n",
    "print(\"\\n\\n... IMPORTS COMPLETE ...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a id=\"background_information\"></a>\n",
    "\n",
    "<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"background_information\">1&nbsp;&nbsp;BACKGROUND INFORMATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.1 BASIC COMPETITION INFORMATION</h3>\n",
    "\n",
    "---\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">PRIMARY TASK DESCRIPTION</b>\n",
    "\n",
    "In this competition, you‚Äôll create a model to automatically segment the stomach and intestines on MRI scans. The MRI scans are from actual cancer patients who had 1-5 MRI scans on separate days during their radiation treatment. You'll base your algorithm on a dataset of these scans to come up with creative deep learning solutions that will help cancer patients get better care.\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">BASIC BACKGROUND INFORMATION</b>\n",
    "\n",
    "In 2019, an estimated 5 million people were diagnosed with a cancer of the gastro-intestinal tract worldwide. Of these patients, about half are eligible for radiation therapy, usually delivered over 10-15 minutes a day for 1-6 weeks. Radiation oncologists try to deliver high doses of radiation using X-ray beams pointed to tumors while avoiding the stomach and intestines. With newer technology such as integrated magnetic resonance imaging and linear accelerator systems, also known as MR-Linacs, <b>oncologists are able to visualize the daily position of the tumor and intestines, <mark>which can vary day to day</mark></b>. \n",
    "\n",
    "In these scans, radiation oncologists must manually outline the position of the stomach and intestines in order to adjust the direction of the x-ray beams to increase the dose delivery to the tumor and avoid the stomach and intestines. This is a time-consuming and labor intensive process that can prolong treatments from 15 minutes a day to an hour a day, which can be difficult for patients to tolerate‚Äîunless deep learning could help automate the segmentation process. <b><mark>A method to segment the stomach and intestines would make treatments much faster and would allow more patients to get more effective treatment.</mark></b>\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">COMPETITION HOST INFORMATION</b>\n",
    "\n",
    "The UW-Madison Carbone Cancer Center is a pioneer in MR-Linac based radiotherapy, and has treated patients with MRI guided radiotherapy based on their daily anatomy since 2015. UW-Madison has generously agreed to support this project which provides anonymized MRIs of patients treated at the UW-Madison Carbone Cancer Center. The University of Wisconsin-Madison is a public land-grant research university in Madison, Wisconsin. The Wisconsin Idea is the university's pledge to the state, the nation, and the world that their endeavors will benefit all citizens.\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">VISUAL EXPLANATION</b>\n",
    "\n",
    "<center><img src=\"https://lh5.googleusercontent.com/zbBUgbj1jyZxyu3r1vr5zKKr8yK1hSdwAM3HpD_n6j2W-5-wKP3ZRusi_3yskSgnC-tMRKqOEtLycbLkTWCJAUe4Cylv_VsW81DYI4ray02uZLeSnlzAuZRIU7L2Q0KURYSMqFI\"></center><br>\n",
    "\n",
    "<i>The tumor above (pink thick line) is close to the stomach (red thick line). High doses of radiation are directed to the tumor while avoiding the stomach. Dose levels are represented by colour. Higher doses are represented by red and lower doses are represented by green.</i><br>\n",
    "\n",
    "<br><center><img src=\"https://www.humonc.wisc.edu/wp-content/uploads/2017/09/Bayouth_Project4_72ppi.png\"></center><br>\n",
    "\n",
    "<i>MRI is an excellent imaging modality for visualization of soft tissues. This is particularly useful for tumors of the abdomen, such as pancreatic cancer shown below.  The left image shows the patient‚Äôs anatomy during exhale, while the image on the right shows the anatomical change during a maximum inspiration breath hold (MIBH). In the MIBH image we can see motion of nearly all the soft tissue, providing us superior ability to align the tumor during our treatment delivery. We are analyzing the clinical impact of using these treatment planning and delivery techniques and our patient‚Äôs ability to comply with self-guided breathing maneuvers.<b><a href=\"https://www.humonc.wisc.edu/research/medical-physics_research/mr-guided-radiation-therapy-research-2/\">[REF]</a></b></i>\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">COMPETITION IMPACT STATEMENT</b>\n",
    "\n",
    "Cancer takes enough of a toll. If successful, you'll enable radiation oncologists to safely deliver higher doses of radiation to tumors while avoiding the stomach and intestines. This will make cancer patients' daily treatments faster and allow them to get more effective treatment with less side effects and better long-term cancer control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.2 COMPETITION EVALUATION</h3>\n",
    "\n",
    "---\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL EVALUATION INFORMATION</b>\n",
    "\n",
    "This competition is evaluated on the mean <a href=\"https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\"><b>Dice coefficient</b></a> and <a href=\"https://github.com/scipy/scipy/blob/master/scipy/spatial/_hausdorff.pyx\"><b>3D Hausdorff distance</b></a>. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n",
    "\n",
    "$$\n",
    "\\frac{2 * |X \\cap Y|}{|X| + |Y|}\n",
    "$$\n",
    "\n",
    "where $X$ is the predicted set of pixels and $Y$ is the ground truth. \n",
    "* The Dice coefficient is defined to be $1$ when both $X$ and $Y$ are empty. \n",
    "* The leaderboard score is the <b>mean of the Dice coefficients for each image in the test set.</b>\n",
    "\n",
    "Hausdorff distance is a method for calculating the distance between segmentation objects A and B, by calculating the furthest point on object A from the nearest point on object B. For 3D Hausdorff, we construct 3D volumes by combining each 2D segmentation with slice depth as the Z coordinate and then find the Hausdorff distance between them. **(In this competition, the slice depth for all scans is set to 1.)** <a href=\"https://github.com/scipy/scipy/blob/master/scipy/spatial/_hausdorff.pyx\"><b>The scipy code for Hausdorff is linked</b></a>. The expected / predicted pixel locations are normalized by image size to create a bounded 0-1 score.\n",
    "\n",
    "<br>\n",
    "    \n",
    "---\n",
    "\n",
    "<b>NOTE: The two metrics are combined during evaluation!</b>\n",
    "\n",
    "* <b>Weight of 0.4 for the Dice metric</b>\n",
    "* <b>Weight of 0.6 for the Hausdorff distance.</b>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">SUBMISSION FILE INFORMATION</b>\n",
    "\n",
    "In order to reduce the submission file size, our metric uses **run-length encoding** on the pixel values.  \n",
    "* Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length\n",
    "* E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n",
    "* Note that, at the time of encoding, the mask should be **binary**\n",
    "    * The masks for all objects in an image are joined into a single large mask\n",
    "    * The value of **0** should indicate pixels that are not **masked**\n",
    "    * The value of **1** will indicate pixels that are **masked**.\n",
    "\n",
    "The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n",
    "\n",
    "<br>\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "id,class,predicted\n",
    "1,large_bowel,1 1 5 1\n",
    "1,small_bowel,1 1\n",
    "1,stomach,1 1\n",
    "2,large_bowel,1 5 2 17\n",
    "etc.\n",
    "```\n",
    "\n",
    "<br><font color=\"red\"><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">IS THIS A CODE COMPETITION?</b></font>\n",
    "\n",
    "<font color=\"red\" style=\"font-size: 30px\"><b>YES</b></font>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.3 DATASET OVERVIEW</h3>\n",
    "\n",
    "---\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL INFORMATION</b>\n",
    "\n",
    "<b><mark>In this competition we are segmenting organs cells in images</mark></b>. \n",
    "\n",
    "The training **<mark>annotations are provided as RLE-encoded masks</mark>**, and the images are in **<mark>16-bit</mark>**, **<mark>grayscale</mark>**, **<mark>PNG format</mark>**.\n",
    "\n",
    "Each case in this competition is represented by multiple sets of scan slices\n",
    "* Each set is identified by the day the scan took place\n",
    "* Some cases are split by time\n",
    "    * early days are in train\n",
    "    * later days are in test\n",
    "* Some cases are split by case\n",
    "    * the entirety of the case is in train or test\n",
    "\n",
    "<b><mark>The goal of this competition is to be able to generalize to both partially and wholly unseen cases.</mark></b>\n",
    "\n",
    "Note that, in this case, the test set is entirely unseen.\n",
    "* It is roughly 50 cases\n",
    "* It contains a varying number of days and slices, (similar to the training set)\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">FILE INFORMATION</b>\n",
    "\n",
    "**`train.csv`** \n",
    "- IDs and masks for all training objects.\n",
    "- **Columns**\n",
    "    * **`id`**\n",
    "        * unique identifier for object\n",
    "    * **`class`**\n",
    "        * the predicted class for the object\n",
    "    * **`EncodedPixels`**\n",
    "        * RLE-encoded pixels for the identified object\n",
    "\n",
    "<br>\n",
    "\n",
    "**`sample_submission.csv`**\n",
    "- A sample submission file in the correct format\n",
    "\n",
    "<br>\n",
    "\n",
    "**`train/`**\n",
    "- a folder of case/day folders, each containing slice images for a particular case on a given day.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n",
    "    <b style=\"font-size: 18px;\">‚ö†Ô∏è &nbsp; NOTE &nbsp; ‚ö†Ô∏è</b><br><br><b style=\"font-size: 22px; color: darkorange\"></b><br><br>The <b>image filenames</b> include 4 numbers <b>(ex. 276_276_1.63_1.63.png)</b>.<br><br>These four numbers are representative of:<ul><li><b>slice height</b> (integer in pixels)</li><li><b>slice width</b> (integer in pixels)</li><li><b>heigh pixel spacing</b> (floating point in mm)</li><li><b>width pixel spacing</b> (floating point in mm)</li></ul><br>The first two defines the resolution of the slide. The last two record the physical size of each pixel.<br><br>\n",
    "</div></center>\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a id=\"background_information\"></a>\n",
    "\n",
    "<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"setup\">2&nbsp;&nbsp;SETUP&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.1 ACCELERATOR DETECTION</h3>\n",
    "\n",
    "---\n",
    "\n",
    "In order to use **`TPU`**, we use **`TPUClusterResolver`** for the initialization which is necessary to connect to the remote cluster and initialize cloud TPUs. Let's go over two important points\n",
    "\n",
    "1. When using TPU on Kaggle, you don't need to specify arguments for **`TPUClusterResolver`**\n",
    "2. However, on **G**oogle **C**ompute **E**ngine (**GCE**), you will need to do the following:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# The name you gave to the TPU to use\n",
    "TPU_WORKER = 'my-tpu-name'\n",
    "\n",
    "# or you can also specify the grpc path directly\n",
    "# TPU_WORKER = 'grpc://xxx.xxx.xxx.xxx:8470'\n",
    "\n",
    "# The zone you chose when you created the TPU to use on GCP.\n",
    "ZONE = 'us-east1-b'\n",
    "\n",
    "# The name of the GCP project where you created the TPU to use on GCP.\n",
    "PROJECT = 'my-tpu-project'\n",
    "\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER, zone=ZONE, project=PROJECT)\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n",
    "    <b style=\"font-size: 16px;\">üõë &nbsp; WARNING:</b><br><br>- Although the Tensorflow documentation says it is the <b>project name</b> that should be provided for the argument <b><code>`project`</code></b>, it is actually the <b>Project ID</b>, that you should provide. This can be found on the GCP project dashboard page.<br>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n",
    "    <b style=\"font-size: 16px;\">üìñ &nbsp; REFERENCES:</b><br><br>\n",
    "    - <a href=\"https://www.tensorflow.org/guide/tpu#tpu_initialization\"><b>Guide - Use TPUs</b></a><br>\n",
    "    - <a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver\"><b>Doc - TPUClusterResolver</b></a><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n... ACCELERATOR SETUP STARTING ...\\n\")\n",
    "\n",
    "print(f\"\\n... RUNNING ON CPU/GPU ...\")\n",
    "# Yield the default distribution strategy in Tensorflow\n",
    "#   --> Works on CPU and single GPU.\n",
    "strategy = tf.distribute.get_strategy() \n",
    "\n",
    "# What Is a Replica?\n",
    "#    --> A single Cloud TPU device consists of FOUR chips, each of which has TWO TPU cores. \n",
    "#    --> Therefore, for efficient utilization of Cloud TPU, a program should make use of each of the EIGHT (4x2) cores. \n",
    "#    --> Each replica is essentially a copy of the training graph that is run on each core and \n",
    "#        trains a mini-batch containing 1/8th of the overall batch size\n",
    "N_REPLICAS = strategy.num_replicas_in_sync\n",
    "    \n",
    "print(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n",
    "\n",
    "print(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.2 COMPETITION DATA ACCESS</h3>\n",
    "\n",
    "---\n",
    "\n",
    "TPUs read data must be read directly from **G**oogle **C**loud **S**torage **(GCS)**. Kaggle provides a utility library ‚Äì¬†**`KaggleDatasets`** ‚Äì which has a utility function **`.get_gcs_path`** that will allow us to access the location of our input datasets within **GCS**.<br><br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n",
    "    <b style=\"font-size: 16px;\">üìå &nbsp; TIPS:</b><br><br>- If you have multiple datasets attached to the notebook, you should pass the name of a specific dataset to the <b><code>`get_gcs_path()`</code></b> function. <i>In our case, the name of the dataset is the name of the directory the dataset is mounted within.</i><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n... DATA ACCESS SETUP STARTED ...\\n\")\n",
    "\n",
    "# Local path to training and validation images\n",
    "DATA_DIR = \"/data/elastic-notebook/data/uw-madison-gi-tract-image-segmentation\"\n",
    "save_locally = None\n",
    "load_locally = None\n",
    "\n",
    "print(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\n",
    "\n",
    "print(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\n",
    "for file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n",
    "\n",
    "print(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.3 LEVERAGING XLA OPTIMIZATIONS</h3>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**XLA** (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes. **The results are improvements in speed and memory usage**.\n",
    "\n",
    "<br>\n",
    "\n",
    "When a TensorFlow program is run, all of the operations are executed individually by the TensorFlow executor. Each TensorFlow operation has a precompiled GPU/TPU kernel implementation that the executor dispatches to.\n",
    "\n",
    "XLA provides us with an alternative mode of running models: it compiles the TensorFlow graph into a sequence of computation kernels generated specifically for the given model. Because these kernels are unique to the model, they can exploit model-specific information for optimization.<br><br>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n",
    "    <b style=\"font-size: 16px;\">üõë &nbsp; WARNING:</b><br><br>- XLA can not currently compile functions where dimensions are not inferrable: that is, if it's not possible to infer the dimensions of all tensors without running the entire computation<br>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n",
    "    <b style=\"font-size: 16px;\">üìå &nbsp; NOTE:</b><br><br>- XLA compilation is only applied to code that is compiled into a graph (in <b>TF2</b> that's only a code inside <b><code>tf.function</code></b>).<br>- The <b><code>jit_compile</code></b> API has must-compile semantics, i.e. either the entire function is compiled with XLA, or an <b><code>errors.InvalidArgumentError</code></b> exception is thrown)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n",
    "    <b style=\"font-size: 16px;\">üìñ &nbsp; REFERENCE:</b><br><br>    - <a href=\"https://www.tensorflow.org/xla\"><b>XLA: Optimizing Compiler for Machine Learning</b></a><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n",
    "\n",
    "print(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n",
    "# enable XLA optmizations (10% speedup when using @tf.function calls)\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "print(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.4 BASIC DATA DEFINITIONS & INITIALIZATIONS</h3>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... BASIC DATA SETUP STARTING ...\n",
      "\n",
      "\n",
      "\n",
      "... ORIGINAL TRAINING DATAFRAME... \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>segmentation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case123_day20_slice_0001</td>\n",
       "      <td>large_bowel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>case123_day20_slice_0001</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case123_day20_slice_0001</td>\n",
       "      <td>stomach</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case123_day20_slice_0002</td>\n",
       "      <td>large_bowel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>case123_day20_slice_0002</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115483</th>\n",
       "      <td>case30_day0_slice_0143</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115484</th>\n",
       "      <td>case30_day0_slice_0143</td>\n",
       "      <td>stomach</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115485</th>\n",
       "      <td>case30_day0_slice_0144</td>\n",
       "      <td>large_bowel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115486</th>\n",
       "      <td>case30_day0_slice_0144</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115487</th>\n",
       "      <td>case30_day0_slice_0144</td>\n",
       "      <td>stomach</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115488 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        class segmentation\n",
       "0       case123_day20_slice_0001  large_bowel          NaN\n",
       "1       case123_day20_slice_0001  small_bowel          NaN\n",
       "2       case123_day20_slice_0001      stomach          NaN\n",
       "3       case123_day20_slice_0002  large_bowel          NaN\n",
       "4       case123_day20_slice_0002  small_bowel          NaN\n",
       "...                          ...          ...          ...\n",
       "115483    case30_day0_slice_0143  small_bowel          NaN\n",
       "115484    case30_day0_slice_0143      stomach          NaN\n",
       "115485    case30_day0_slice_0144  large_bowel          NaN\n",
       "115486    case30_day0_slice_0144  small_bowel          NaN\n",
       "115487    case30_day0_slice_0144      stomach          NaN\n",
       "\n",
       "[115488 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "... ORIGINAL SUBMISSION DATAFRAME... \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, class, predicted]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "... DEBUG SUBMISSION DATAFRAME... \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case123_day20_slice_0001</td>\n",
       "      <td>large_bowel</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>case123_day20_slice_0001</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case123_day20_slice_0001</td>\n",
       "      <td>stomach</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case123_day20_slice_0002</td>\n",
       "      <td>large_bowel</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>case123_day20_slice_0002</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>case123_day20_slice_0002</td>\n",
       "      <td>stomach</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>case123_day20_slice_0003</td>\n",
       "      <td>large_bowel</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>case123_day20_slice_0003</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>case123_day20_slice_0003</td>\n",
       "      <td>stomach</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>case123_day20_slice_0004</td>\n",
       "      <td>large_bowel</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id        class predicted\n",
       "0  case123_day20_slice_0001  large_bowel          \n",
       "1  case123_day20_slice_0001  small_bowel          \n",
       "2  case123_day20_slice_0001      stomach          \n",
       "3  case123_day20_slice_0002  large_bowel          \n",
       "4  case123_day20_slice_0002  small_bowel          \n",
       "5  case123_day20_slice_0002      stomach          \n",
       "6  case123_day20_slice_0003  large_bowel          \n",
       "7  case123_day20_slice_0003  small_bowel          \n",
       "8  case123_day20_slice_0003      stomach          \n",
       "9  case123_day20_slice_0004  large_bowel          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "... ARE WE DEBUGGING: True... \n",
      "\n",
      "\n",
      "... BASIC DATA SETUP FINISHED ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\n",
    "\n",
    "# Open the training dataframe and display the initial dataframe\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# Get all training images\n",
    "all_train_images = glob(os.path.join(TRAIN_DIR, \"**\", \"*.png\"), recursive=True)\n",
    "\n",
    "print(\"\\n... ORIGINAL TRAINING DATAFRAME... \\n\")\n",
    "display(train_df)\n",
    "\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "SS_CSV   = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "ss_df = pd.read_csv(SS_CSV)\n",
    "\n",
    "# Get all testing images if there are any\n",
    "all_test_images = glob(os.path.join(TEST_DIR, \"**\", \"*.png\"), recursive=True)\n",
    "\n",
    "print(\"\\n\\n\\n... ORIGINAL SUBMISSION DATAFRAME... \\n\")\n",
    "display(ss_df)\n",
    "\n",
    "# For debugging purposes when the test set hasn't been substituted we will know\n",
    "DEBUG=len(ss_df)==0\n",
    "\n",
    "if DEBUG:\n",
    "    TEST_DIR = TRAIN_DIR\n",
    "    all_test_images = all_train_images\n",
    "    ss_df = train_df.iloc[:10]\n",
    "    ss_df = ss_df[[\"id\", \"class\"]]\n",
    "    ss_df[\"predicted\"] = \"\"\n",
    "    \n",
    "    print(\"\\n\\n\\n... DEBUG SUBMISSION DATAFRAME... \\n\")\n",
    "    display(ss_df)\n",
    "\n",
    "    \n",
    "\n",
    "SF2LF = {\"lb\":\"Large Bowel\",\"sb\":\"Small Bowel\",\"st\":\"Stomach\"}\n",
    "LF2SF = {v:k for k,v in SF2LF.items()}\n",
    "print(f\"\\n\\n\\n... ARE WE DEBUGGING: {DEBUG}... \\n\")\n",
    "\n",
    "print(\"\\n... BASIC DATA SETUP FINISHED ...\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.5 UPDATE DATAFRAMES WITH ACCESSIBLE EXTRA INFORMATION</h3>\n",
    "\n",
    "---\n",
    "\n",
    "I wrapped the logic in a preprocessing function but also went through step by step so people could validate if they so wished\n",
    "\n",
    "**NOTE: I have changed the column identifiers as follows for the sake of brevity:**\n",
    "* **large_bowel** --> **lb**\n",
    "* **small_bowel** --> **sb**\n",
    "* **stomach** --> **st**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepath_from_partial_identifier(_ident, file_list):\n",
    "    return [x for x in file_list if _ident in x][0]\n",
    "\n",
    "def df_preprocessing(df, globbed_file_list, is_test=False):\n",
    "    \"\"\" The preprocessing steps applied to get column information \"\"\"\n",
    "    # 1. Get Case-ID as a column (str and int)\n",
    "    df[\"case_id_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n",
    "    df[\"case_id\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n",
    "\n",
    "    # 2. Get Day as a column\n",
    "    df[\"day_num_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n",
    "    df[\"day_num\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n",
    "\n",
    "    # 3. Get Slice Identifier as a column\n",
    "    df[\"slice_id\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n",
    "\n",
    "    # 4. Get full file paths for the representative scans\n",
    "    df[\"_partial_ident\"] = (globbed_file_list[0].rsplit(\"/\", 4)[0]+\"/\"+ # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n",
    "                           df[\"case_id_str\"]+\"/\"+ # .../case###/\n",
    "                           df[\"case_id_str\"]+\"_\"+df[\"day_num_str\"]+ # .../case###_day##/\n",
    "                           \"/scans/\"+df[\"slice_id\"]) # .../slice_#### \n",
    "    _tmp_merge_df = pd.DataFrame({\"_partial_ident\":[x.rsplit(\"_\",4)[0] for x in globbed_file_list], \"f_path\":globbed_file_list})\n",
    "    df = df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n",
    "\n",
    "    # 5. Get slice dimensions from filepath (int in pixels)\n",
    "    df[\"slice_h\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n",
    "    df[\"slice_w\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n",
    "\n",
    "    # 6. Pixel spacing from filepath (float in mm)\n",
    "    df[\"px_spacing_h\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[3]))\n",
    "    df[\"px_spacing_w\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[4]))\n",
    "\n",
    "    if not is_test:\n",
    "        # 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\n",
    "        l_bowel_df = df[df[\"class\"]==\"large_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"lb_seg_rle\"})\n",
    "        s_bowel_df = df[df[\"class\"]==\"small_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"sb_seg_rle\"})\n",
    "        stomach_df = df[df[\"class\"]==\"stomach\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"st_seg_rle\"})\n",
    "        df = df.merge(l_bowel_df, on=\"id\", how=\"left\")\n",
    "        df = df.merge(s_bowel_df, on=\"id\", how=\"left\")\n",
    "        df = df.merge(stomach_df, on=\"id\", how=\"left\")\n",
    "        df = df.drop_duplicates(subset=[\"id\",]).reset_index(drop=True)\n",
    "        df[\"lb_seg_flag\"] = df[\"lb_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "        df[\"sb_seg_flag\"] = df[\"sb_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "        df[\"st_seg_flag\"] = df[\"st_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "        df[\"n_segs\"] = df[\"lb_seg_flag\"].astype(int)+df[\"sb_seg_flag\"].astype(int)+df[\"st_seg_flag\"].astype(int)\n",
    "\n",
    "    # 8. Reorder columns to the a new ordering (drops class and segmentation as no longer necessary)\n",
    "    new_col_order = [\"id\", \"f_path\", \"n_segs\",\n",
    "                     \"lb_seg_rle\", \"lb_seg_flag\",\n",
    "                     \"sb_seg_rle\", \"sb_seg_flag\", \n",
    "                     \"st_seg_rle\", \"st_seg_flag\",\n",
    "                     \"slice_h\", \"slice_w\", \"px_spacing_h\", \n",
    "                     \"px_spacing_w\", \"case_id_str\", \"case_id\", \n",
    "                     \"day_num_str\", \"day_num\", \"slice_id\",]\n",
    "    if is_test: new_col_order.insert(1, \"class\")\n",
    "    new_col_order = [_c for _c in new_col_order if _c in df.columns]\n",
    "    df = df[new_col_order]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n... UPDATING DATAFRAMES WITH ACCESSIBLE INFORMATION STARTED ...\\n\\n\")\n",
    "\n",
    "# 1. Get Case-ID as a column (str and int)\n",
    "train_df[\"case_id_str\"] = train_df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n",
    "train_df[\"case_id\"] = train_df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n",
    "\n",
    "# 2. Get Day as a column\n",
    "train_df[\"day_num_str\"] = train_df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n",
    "train_df[\"day_num\"] = train_df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n",
    "\n",
    "# 3. Get Slice Identifier as a column\n",
    "train_df[\"slice_id\"] = train_df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n",
    "\n",
    "# 4. Get full file paths for the representative scans\n",
    "train_df[\"_partial_ident\"] = (TRAIN_DIR+\"/\"+ # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n",
    "                             train_df[\"case_id_str\"]+\"/\"+ # .../case###/\n",
    "                             train_df[\"case_id_str\"]+\"_\"+train_df[\"day_num_str\"]+ # .../case###_day##/\n",
    "                             \"/scans/\"+train_df[\"slice_id\"]) # .../slice_#### \n",
    "_tmp_merge_df = pd.DataFrame({\"_partial_ident\":[x.rsplit(\"_\",4)[0] for x in all_train_images], \"f_path\":all_train_images})\n",
    "train_df = train_df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n",
    "\n",
    "# Minor cleanup of our temporary workaround\n",
    "del _tmp_merge_df; gc.collect(); gc.collect()\n",
    "\n",
    "# 5. Get slice dimensions from filepath (int in pixels)\n",
    "train_df[\"slice_h\"] = train_df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n",
    "train_df[\"slice_w\"] = train_df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n",
    "\n",
    "# 6. Pixel spacing from filepath (float in mm)\n",
    "train_df[\"px_spacing_h\"] = train_df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[3]))\n",
    "train_df[\"px_spacing_w\"] = train_df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[4]))\n",
    "\n",
    "# 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\n",
    "l_bowel_train_df = train_df[train_df[\"class\"]==\"large_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"lb_seg_rle\"})\n",
    "s_bowel_train_df = train_df[train_df[\"class\"]==\"small_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"sb_seg_rle\"})\n",
    "stomach_train_df = train_df[train_df[\"class\"]==\"stomach\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"st_seg_rle\"})\n",
    "train_df = train_df.merge(l_bowel_train_df, on=\"id\", how=\"left\")\n",
    "train_df = train_df.merge(s_bowel_train_df, on=\"id\", how=\"left\")\n",
    "train_df = train_df.merge(stomach_train_df, on=\"id\", how=\"left\")\n",
    "train_df = train_df.drop_duplicates(subset=[\"id\",]).reset_index(drop=True)\n",
    "train_df[\"lb_seg_flag\"] = train_df[\"lb_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "train_df[\"sb_seg_flag\"] = train_df[\"sb_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "train_df[\"st_seg_flag\"] = train_df[\"st_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "train_df[\"n_segs\"] = train_df[\"lb_seg_flag\"].astype(int)+train_df[\"sb_seg_flag\"].astype(int)+train_df[\"st_seg_flag\"].astype(int)\n",
    "\n",
    "# 8. Reorder columns to the a new ordering (drops class and segmentation as no longer necessary)\n",
    "train_df = train_df[[\"id\", \"f_path\", \"n_segs\",\n",
    "                     \"lb_seg_rle\", \"lb_seg_flag\",\n",
    "                     \"sb_seg_rle\", \"sb_seg_flag\", \n",
    "                     \"st_seg_rle\", \"st_seg_flag\",\n",
    "                     \"slice_h\", \"slice_w\", \"px_spacing_h\", \n",
    "                     \"px_spacing_w\", \"case_id_str\", \"case_id\", \n",
    "                     \"day_num_str\", \"day_num\", \"slice_id\",]]\n",
    "\n",
    "# 9. Display update dataframe\n",
    "print(\"\\n... UPDATED TRAINING DATAFRAME... \\n\")\n",
    "display(train_df)\n",
    "\n",
    "ss_df = df_preprocessing(ss_df, all_test_images, is_test=True)\n",
    "print(\"\\n\\n\\n... UPDATED SUBMISSION DATAFRAME... \\n\")\n",
    "display(ss_df)\n",
    "\n",
    "print(\"\\n... UPDATING DATAFRAMES WITH ACCESSIBLE INFORMATION FINISHED ...\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "<a id=\"helper_functions\"></a>\n",
    "\n",
    "\n",
    "<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"helper_functions\">\n",
    "    3&nbsp;&nbsp;HELPER FUNCTION & CLASSES&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n",
    "</h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "# modified from: https://www.kaggle.com/inversion/run-length-decoding-quick-start\n",
    "def rle_decode(mask_rle, shape, color=1):\n",
    "    \"\"\" TBD\n",
    "    \n",
    "    Args:\n",
    "        mask_rle (str): run-length as string formated (start length)\n",
    "        shape (tuple of ints): (height,width) of array to return \n",
    "    \n",
    "    Returns: \n",
    "        Mask (np.array)\n",
    "            - 1 indicating mask\n",
    "            - 0 indicating background\n",
    "\n",
    "    \"\"\"\n",
    "    # Split the string by space, then convert it into a integer array\n",
    "    s = np.array(mask_rle.split(), dtype=int)\n",
    "\n",
    "    # Every even value is the start, every odd value is the \"run\" length\n",
    "    starts = s[0::2] - 1\n",
    "    lengths = s[1::2]\n",
    "    ends = starts + lengths\n",
    "\n",
    "    # The image image is actually flattened since RLE is a 1D \"run\"\n",
    "    if len(shape)==3:\n",
    "        h, w, d = shape\n",
    "        img = np.zeros((h * w, d), dtype=np.float32)\n",
    "    else:\n",
    "        h, w = shape\n",
    "        img = np.zeros((h * w,), dtype=np.float32)\n",
    "\n",
    "    # The color here is actually just any integer you want!\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo : hi] = color\n",
    "        \n",
    "    # Don't forget to change the image back to the original shape\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# https://www.kaggle.com/namgalielei/which-reshape-is-used-in-rle\n",
    "def rle_decode_top_to_bot_first(mask_rle, shape):\n",
    "    \"\"\" TBD\n",
    "    \n",
    "    Args:\n",
    "        mask_rle (str): run-length as string formated (start length)\n",
    "        shape (tuple of ints): (height,width) of array to return \n",
    "    \n",
    "    Returns:\n",
    "        Mask (np.array)\n",
    "            - 1 indicating mask\n",
    "            - 0 indicating background\n",
    "\n",
    "    \"\"\"\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape((shape[1], shape[0]), order='F').T  # Reshape from top -> bottom first\n",
    "\n",
    "# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n",
    "def rle_encode(img):\n",
    "    \"\"\" TBD\n",
    "    \n",
    "    Args:\n",
    "        img (np.array): \n",
    "            - 1 indicating mask\n",
    "            - 0 indicating background\n",
    "    \n",
    "    Returns: \n",
    "        run length as string formated\n",
    "    \"\"\"\n",
    "    \n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def flatten_l_o_l(nested_list):\n",
    "    \"\"\" Flatten a list of lists \"\"\"\n",
    "    return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "def load_json_to_dict(json_path):\n",
    "    \"\"\" tbd \"\"\"\n",
    "    with open(json_path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "def tf_load_png(img_path):\n",
    "    return tf.image.decode_png(tf.io.read_file(img_path), channels=3)\n",
    "\n",
    "def open_gray16(_path, normalize=True, to_rgb=False):\n",
    "    \"\"\" Helper to open files \"\"\"\n",
    "    if normalize:\n",
    "        if to_rgb:\n",
    "            return np.tile(np.expand_dims(cv2.imread(_path, cv2.IMREAD_ANYDEPTH)/65535., axis=-1), 3)\n",
    "        else:\n",
    "            return cv2.imread(_path, cv2.IMREAD_ANYDEPTH)/65535.\n",
    "    else:\n",
    "        if to_rgb:\n",
    "            return np.tile(np.expand_dims(cv2.imread(_path, cv2.IMREAD_ANYDEPTH), axis=-1), 3)\n",
    "        else:\n",
    "            return cv2.imread(_path, cv2.IMREAD_ANYDEPTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "<a id=\"create_dataset\"></a>\n",
    "\n",
    "\n",
    "<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"create_dataset\">\n",
    "    4&nbsp;&nbsp;DATASET EXPLORATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n",
    "</h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.0 LOOK AT A SINGLE EXAMPLE PRIOR TO INVESTIGATION</h3>\n",
    "\n",
    "---\n",
    "\n",
    "We simply do this to make sure everything is where it should be and we understand the basics of how to access all the relevant data.\n",
    "\n",
    "We will wrap this basic exploration functionality as single function to allow for easy examination of any passed identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlay(img_path, rle_strs, img_shape, _alpha=0.999, _beta=0.35, _gamma=0):\n",
    "    _img = open_gray16(img_path, to_rgb=True)\n",
    "    _img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n",
    "    _seg_rgb = np.stack([rle_decode(rle_str, shape=img_shape, color=1) if rle_str is not None else np.zeros(img_shape, dtype=np.float32) for rle_str in rle_strs], axis=-1).astype(np.float32)\n",
    "    seg_overlay = cv2.addWeighted(src1=_img, alpha=_alpha, \n",
    "                                  src2=_seg_rgb, beta=_beta, gamma=_gamma)\n",
    "    return seg_overlay\n",
    "\n",
    "def examine_id(ex_id, df=train_df, plot_overlay=True, print_meta=False, plot_grayscale=False, plot_binary_segmentation=False):\n",
    "    \"\"\" Wrapper function to allow for easy visual exploration of an example \"\"\"\n",
    "    print(f\"\\n... ID ({ex_id}) EXPLORATION STARTED ...\\n\\n\")\n",
    "    demo_ex = df[df.id==ex_id].squeeze()\n",
    "\n",
    "    if print_meta:\n",
    "        print(f\"\\n... WITH DEMO_ID=`{DEMO_ID}` WE HAVE THE FOLLOWING DEMO EXAMPLE TO WORK FROM... \\n\\n\")\n",
    "        display(demo_ex.to_frame())\n",
    "\n",
    "    if plot_grayscale:\n",
    "        print(f\"\\n\\n... GRAYSCALE IMAGE PLOT ...\\n\")\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(open_gray16(demo_ex.f_path), cmap=\"gray\")\n",
    "        plt.title(f\"Original Grayscale Image For ID: {demo_ex.id}\", fontweight=\"bold\")\n",
    "        plt.axis(False)\n",
    "        plt.show()\n",
    "\n",
    "    if plot_binary_segmentation:\n",
    "        print(f\"\\n\\n... BINARY SEGMENTATION MASKS ...\\n\")\n",
    "        plt.figure(figsize=(20,10))\n",
    "        for i, _seg_type in enumerate([\"lb\", \"sb\", \"st\"]):\n",
    "            if pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]): continue\n",
    "            plt.subplot(1,3,i+1)\n",
    "            plt.imshow(rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1))\n",
    "            plt.title(f\"RLE Encoding For {SF2LF[_seg_type]} Segmentation\", fontweight=\"bold\")\n",
    "            plt.axis(False)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if plot_overlay:\n",
    "        print(f\"\\n\\n... IMAGE WITH RGB SEGMENTATION MASK OVERLAY ...\\n\")\n",
    "        # We need to normalize the loaded image values to be between 0 and 1 or else our plot will look weird\n",
    "        # _img = open_gray16(demo_ex.f_path, to_rgb=True)\n",
    "        #_img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n",
    "        #_seg_rgb = np.stack([rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1) if not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else np.zeros((demo_ex.slice_w, demo_ex.slice_h)) for _seg_type in [\"lb\", \"sb\", \"st\"]], axis=-1).astype(np.float32)\n",
    "        #seg_overlay = cv2.addWeighted(src1=_img, alpha=0.99, \n",
    "                                      #src2=_seg_rgb, beta=0.33, gamma=0)\n",
    "        _rle_strs = [demo_ex[f\"{_seg_type}_seg_rle\"] if not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else None for _seg_type in [\"lb\", \"sb\", \"st\"]]\n",
    "        seg_overlay = get_overlay(demo_ex.f_path, _rle_strs, img_shape=(demo_ex.slice_w, demo_ex.slice_h))\n",
    "\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(seg_overlay)\n",
    "        plt.title(f\"Segmentation Overlay For ID: {demo_ex.id}\", fontweight=\"bold\")\n",
    "        handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n",
    "        labels = [\"Large Bowel Segmentation Map\", \"Small Bowel Segmentation Map\", \"Stomach Segmentation Map\"]\n",
    "        plt.legend(handles,labels)\n",
    "        plt.axis(False)\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\n\\n... SINGLE ID EXPLORATION FINISHED ...\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n... SINGLE ID EXPLORATION STARTED ...\\n\\n\")\n",
    "\n",
    "DEMO_ID = \"case123_day20_slice_0082\"\n",
    "demo_ex = train_df[train_df.id==DEMO_ID].squeeze()\n",
    "\n",
    "print(f\"\\n... WITH DEMO_ID=`{DEMO_ID}` WE HAVE THE FOLLOWING DEMO EXAMPLE TO WORK FROM... \\n\\n\")\n",
    "display(demo_ex.to_frame())\n",
    "\n",
    "print(f\"\\n\\n... LET'S PLOT THE IMAGE FIRST ...\\n\")\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(open_gray16(demo_ex.f_path), cmap=\"gray\")\n",
    "plt.title(f\"Original Grayscale Image For ID: {demo_ex.id}\", fontweight=\"bold\")\n",
    "plt.axis(False)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\\n... LET'S PLOT THE 3 SEGMENTATION MASKS ...\\n\")\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i, _seg_type in enumerate([\"lb\", \"sb\", \"st\"]):\n",
    "    if pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]): continue\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.imshow(rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1))\n",
    "    plt.title(f\"RLE Encoding For {SF2LF[_seg_type]} Segmentation\", fontweight=\"bold\")\n",
    "    plt.axis(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\\n... LET'S PLOT THE IMAGE WITH AN RGB SEGMENTATION MASK OVERLAY ...\\n\")\n",
    "\n",
    "# We need to normalize the loaded image values to be between 0 and 1 or else our plot will look weird\n",
    "_img = open_gray16(demo_ex.f_path, to_rgb=True)\n",
    "_img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n",
    "_seg_rgb = np.stack([rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1) if not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else np.zeros((demo_ex.slice_w, demo_ex.slice_h)) for _seg_type in [\"lb\", \"sb\", \"st\"]], axis=-1).astype(np.float32)\n",
    "seg_overlay = cv2.addWeighted(src1=_img, alpha=0.99, \n",
    "                              src2=_seg_rgb, beta=0.33, gamma=0.0)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(seg_overlay)\n",
    "plt.title(f\"Segmentation Overlay For ID: {demo_ex.id}\", fontweight=\"bold\")\n",
    "handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n",
    "labels = [\"Large Bowel Segmentation Map\", \"Small Bowel Segmentation Map\", \"Stomach Segmentation Map\"]\n",
    "plt.legend(handles,labels)\n",
    "plt.axis(False)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\\n... LET'S PRINT THE RELEVANT INFORMATION ...\\n\")\n",
    "print(f\"\\t--> IMAGE CASE ID              : {demo_ex.case_id}\")\n",
    "print(f\"\\t--> IMAGE DAY NUMBER           : {demo_ex.day_num}\")\n",
    "print(f\"\\t--> IMAGE SLICE WIDTH          : {demo_ex.slice_w}\")\n",
    "print(f\"\\t--> IMAGE SLICE HEIGHT         : {demo_ex.slice_h}\")\n",
    "print(f\"\\t--> IMAGE PIXEL SPACING WIDTH  : {demo_ex.px_spacing_w}\")\n",
    "print(f\"\\t--> IMAGE PIXEL SPACING HEIGHT : {demo_ex.px_spacing_h}\")\n",
    "\n",
    "print(\"\\n\\n... SINGLE ID EXPLORATION FINISHED ...\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since we made the function... let's iterate over some examples where there is a tumor present in all locales and plot them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 10 random-ids where all tumor locales are present (max one id per case)\n",
    "N_TO_PLOT = 10\n",
    "for _id in train_df[train_df.n_segs==3].groupby(\"case_id\")[\"id\"].first().sample(N_TO_PLOT):\n",
    "    examine_id(_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.1 INVESTIGATE THE OCCURENCE SEGMENTATION MAP TYPES</h3>\n",
    "\n",
    "---\n",
    "\n",
    "It's quite apparent that not all images have segmentation maps for the various regions (stomach, large-bowel, small-bowel), so we will identify the frequency for which these occur independently... as well as the frequency for which these maps co-occur.\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n",
    "\n",
    "* There are **38,496** total examples.\n",
    "* It can be observed that more than half of the given examples have no annotations present!\n",
    "    * There are **21,906** (56.9046%) examples with no annotations/masks/segmentation present\n",
    "    * Inversely there are **16,590** (43.0954%) examples with one or more annotations present\n",
    "* There are **2,468** (6.41%) examples with **one annotation present**. \n",
    "* It can be observed that the vast majority of single mask annotations are **Stomach**!\n",
    "    * Of these annotations, **2286** (~92.6%) are **Stomach**\n",
    "    * Of these annotations, **123** (~4.98%) are **Large Bowel**\n",
    "    * Of these annotations, **59** (~2.39%) are **Small Bowel**\n",
    "* There are **10,921** (28.37%) examples with **two annotations present**. \n",
    "* It can be observed, in contrast to the single annotation examples, that the majority of annotations do NOT include stomach i.e. **'Large Bowel, Small Bowel'**!\n",
    "    * Of these annotations, **7781** (~71.3%) are **'Large Bowel, Small Bowel'**\n",
    "    * Of these annotations, **2980** (~27.3%) are **'Large Bowel, Stomach'**\n",
    "    * Of these annotations, **160** (~1.47%) are **'Small Bowel, Stomach'**\n",
    "* Finally, there are **3,201** (8.32%) examples with **all three annotations present**. \n",
    "\n",
    "<!--  # print(len(train_df))\n",
    "# print(len(train_df[train_df[\"seg_combo_str\"]==\"No Mask\"]), len(train_df[train_df[\"seg_combo_str\"]==\"No Mask\"])/len(train_df))\n",
    "# print(len(train_df[train_df[\"seg_combo_str\"]==\"Large Bowel\"])/2468)\n",
    "# print(len(train_df[train_df[\"seg_combo_str\"]==\"Small Bowel\"])/2468)\n",
    "# print(len(train_df[train_df[\"seg_combo_str\"]==\"Stomach\"])/2468)\n",
    "# print(len(train_df[train_df[\"seg_combo_str\"].apply(lambda x: x.count(\",\")==1)]))\n",
    "# print(len(train_df[train_df[\"seg_combo_str\"]==\"Large Bowel, Stomach\"])/10921)\n",
    "# print(len(train_df[train_df[\"seg_combo_str\"]==\"Large Bowel, Small Bowel\"])/10921)\n",
    "# print(len(train_df[train_df[\"seg_combo_str\"]==\"Small Bowel, Stomach\"])/10921)\n",
    "# print(len(train_df[train_df[\"seg_combo_str\"].apply(lambda x: x.count(\",\")==2)])) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seg_combo_str(row):\n",
    "    seg_str_list = []\n",
    "    if row[\"lb_seg_flag\"]: seg_str_list.append(\"Large Bowel\")\n",
    "    if row[\"sb_seg_flag\"]: seg_str_list.append(\"Small Bowel\")\n",
    "    if row[\"st_seg_flag\"]: seg_str_list.append(\"Stomach\")\n",
    "    if len(seg_str_list)>0:\n",
    "        return \", \".join(seg_str_list)\n",
    "    else:\n",
    "        return \"No Mask\"\n",
    "train_df[\"seg_combo_str\"] = train_df.progress_apply(get_seg_combo_str, axis=1)\n",
    "\n",
    "fig = px.histogram(train_df, train_df[\"n_segs\"].astype(str), color=\"seg_combo_str\", title=\"<b>Number of Segmentation Masks Per Image</b>\", \n",
    "                  labels={\"x\":\"Number of Segmentation Masks Per Image\", \"seg_combo_str\":\"<b>Segmentation Masks Present</b>\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.2 INVESTIGATE THE IMAGE SIZES</h3>\n",
    "\n",
    "---\n",
    "\n",
    "It's observable that not all images have the same size... however, given that, there is not that much variation between image slice sizes.\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n",
    "* Remember, there are **38,496** total examples.\n",
    "* Globally, we can see that 3 of the image shapes are **square** while one is **rectangular** and they all fall within a fairly tight distribution of relatively small sizes\n",
    "* Of these there are **4** unique sizes:\n",
    "    * $234 \\times 234$\n",
    "        * **Least frequent** image size\n",
    "        * **Smallest** image size\n",
    "        * Only **144** of the 38,496 occurences are this size (0.37%)\n",
    "    * $266 \\times 266$\n",
    "        * **Most frequent** image size\n",
    "        * **Second smallest** image size\n",
    "        * **25,920** of the 38,496 occurences are this size (67.33%)\n",
    "    * $276 \\times 276$\n",
    "        * **Second least frequent** image size\n",
    "        * **Second largest** image size\n",
    "        * **1,200** of the 38,496 occurences are this size (3.12%)\n",
    "    * $310 \\times 360$\n",
    "        * **Second most frequent** image size\n",
    "        * **Largest** image size\n",
    "        * **11,232** of the 38,496 occurences are this size (29.17%)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(train_df.drop_duplicates(subset=[\"slice_w\", \"slice_h\"]), x=\"slice_w\", y=\"slice_h\", \n",
    "                 size=train_df.groupby([\"slice_w\", \"slice_h\"])[\"id\"].transform(\"count\").iloc[train_df.drop_duplicates(subset=[\"slice_w\", \"slice_h\"]).index], \n",
    "                 color=\"(\"+train_df.drop_duplicates(subset=[\"slice_w\", \"slice_h\"])[\"slice_w\"].astype(str)+\",\"+train_df.drop_duplicates(subset=[\"slice_w\", \"slice_h\"])[\"slice_h\"].astype(str)+\")\", \n",
    "                 title=\"<b>Bubble Chart Showing The Various Image Sizes</b>\",\n",
    "                 labels={\"color\":\"<b>Size Legend</b>\", \n",
    "                         \"size\":\"<b>Number Of Observations</b>\",\n",
    "                         \"slice_h\":\"<b>Image Slice Height (pixels)</b>\",\n",
    "                         \"slice_w\":\"<b>Image Slice Width (pixels)</b>\"},\n",
    "                 size_max=160)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.3 INVESTIGATE THE PIXEL SPACING</h3>\n",
    "\n",
    "---\n",
    "\n",
    "It's observable that not all images have the same pixel spacing... however, given that, there is not that much variation between pixel spacing.\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n",
    "* Remember, there are **38,496** total examples.\n",
    "* Globally, we can see that all of the pixel spacings are **square** and that the vast majority are $1.50mm \\times 1.50mm$\n",
    "* There are only **2** unique sets of pixel spacings:\n",
    "    * $1.50mm \\times 1.50mm$\n",
    "        * **Most frequent** pixel spacing\n",
    "        * **Smallest** pixel spacing (barely)\n",
    "        * **37,296** of the 38,496 occurences are this size (96.88%)\n",
    "    * $1.63mm \\times 1.63mm$\n",
    "        * **Least frequent** image size\n",
    "        * **Largest** pixel spacing (barely)\n",
    "        * **1,200** of the 38,496 occurences are this size (3.12%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(train_df.drop_duplicates(subset=[\"px_spacing_w\", \"px_spacing_h\"]), x=\"px_spacing_w\", y=\"px_spacing_h\", \n",
    "                 size=train_df.groupby([\"px_spacing_w\", \"px_spacing_h\"])[\"id\"].transform(\"count\").iloc[train_df.drop_duplicates(subset=[\"px_spacing_w\", \"px_spacing_h\"]).index], \n",
    "                 color=\"(\"+train_df.drop_duplicates(subset=[\"px_spacing_w\", \"px_spacing_h\"])[\"px_spacing_w\"].astype(str)+\",\"+train_df.drop_duplicates(subset=[\"px_spacing_w\", \"px_spacing_h\"])[\"px_spacing_h\"].astype(str)+\")\", \n",
    "                 title=\"<b>Bubble Chart Showing The Various Pixel Spacings</b>\",\n",
    "                 labels={\"color\":\"<b>Pixel Spacing Sets Legend</b>\", \n",
    "                         \"size\":\"<b>Number Of Observations</b>\",\n",
    "                         \"px_spacing_h\":\"<b>Pixel Spacing Height (mm)</b>\",\n",
    "                         \"px_spacing_w\":\"<b>Pixel Spacing Width (mm)</b>\"},\n",
    "                 size_max=160)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.4 INVESTIGATE CASE IDS</h3>\n",
    "\n",
    "---\n",
    "\n",
    "Here's the host description of **`case_id`**\n",
    "\n",
    "> \"Each case in this competition is represented by multiple sets of scan slices (each set is identified by the day the scan took place). Some cases are split by time (early days are in train, later days are in test) while some cases are split by case - the entirety of the case is in train or test. The goal of this competition is to be able to generalize to both partially and wholly unseen cases.\"\n",
    "\n",
    "I don't really observe any oddities associated with any particular **`case_id`** values. I would probably attempt to group them when stratifying/creating-folds... however, they don't seem to perpetrate an obvious bias.\n",
    "\n",
    "When we colour by **day**, we can see that all cases are made up (mostly) of groups of **144**, or less frequently, **80**, images from different days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(train_df, train_df.case_id.astype(str), color=\"day_num_str\", title=\"<b>Distribution Of Images Per Case ID</b>\", \n",
    "             labels={\"x\":\"<b>Case ID</b>\", \"day_num_str\": \"<b>The Day The Scan Took Place</b>\"}, text_auto=True, width=2000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "def plot_case(case_id, day=None, df=train_df, _figsize=(20,30), n_cols=16):\n",
    "    # Initialize\n",
    "    case_df = df[df.case_id==case_id]\n",
    "    \n",
    "    if day is not None:\n",
    "        _case_df = case_df[(case_df.day_num==day) | (case_df.day_num_str==str(day))]\n",
    "        if len(_case_df)>0:\n",
    "            approx_shrink = len(_case_df)/len(case_df)\n",
    "            case_df=_case_df\n",
    "            _figsize = (_figsize[0], int(np.ceil(1.25*_figsize[1]*approx_shrink)))\n",
    "        else:\n",
    "            print(\"There are no valid samples for the passed `day`. Reverting to all days in case.\")\n",
    "        del _case_df\n",
    "    \n",
    "    n_ex = len(case_df)\n",
    "    \n",
    "    print(\"...Preparing...\")\n",
    "    # Get relevant data\n",
    "    case_paths = case_df[\"f_path\"].tolist()\n",
    "    case_rles = [[_rle if not pd.isna(_rle) else None for _rle in _rles] for _rles in case_df[[\"lb_seg_rle\", \"sb_seg_rle\", \"st_seg_rle\"]].values.tolist()]\n",
    "    case_img_shapes = [(_w,_h) for _w,_h in zip(case_df[\"slice_w\"].tolist(), case_df[\"slice_h\"].tolist())]\n",
    "    all_overlays = [get_overlay(img_path, rle_strs, img_shape) for img_path, rle_strs, img_shape in zip(case_paths, case_rles, case_img_shapes)]\n",
    "    \n",
    "    print(\"...Plotting...\")    \n",
    "    # Plot\n",
    "    plt.figure(figsize=_figsize)\n",
    "    n_rows = int(np.ceil(n_ex/n_cols))\n",
    "    \n",
    "    gs = gridspec.GridSpec(n_rows, n_cols,\n",
    "         wspace=0.0, hspace=0.0, \n",
    "         top=1.-0.5/(n_rows+1), bottom=0.5/(n_rows+1), \n",
    "         left=0.5/(n_cols+1), right=1-0.5/(n_cols+1))\n",
    "    \n",
    "    for i in range(n_rows):\n",
    "        if len(all_overlays)==0: break\n",
    "        for j in range(n_cols):\n",
    "            if len(all_overlays)==0: break\n",
    "            ax=plt.subplot(gs[i,j])\n",
    "            ax.imshow(all_overlays.pop())\n",
    "            ax.axis(False)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "        \n",
    "    print(\"...Displaying...\")    \n",
    "    plt.show()\n",
    "        \n",
    "DEMO_CASE = 134\n",
    "print(f\"\\n\\n... PLOTTING DEMO CASE ID #{DEMO_CASE} ...\\n\\n\")\n",
    "plot_case(DEMO_CASE)\n",
    "\n",
    "DEMO_CASE = 9\n",
    "print(f\"\\n\\n\\n\\n... PLOTTING DEMO CASE ID #{DEMO_CASE} ...\\n\\n\")\n",
    "plot_case(DEMO_CASE)\n",
    "\n",
    "DEMO_CASE = 7\n",
    "DEMO_DAYS = [0, 13, 19]\n",
    "for _dday in DEMO_DAYS:\n",
    "    print(f\"\\n\\n\\n... PLOTTING DEMO CASE ID #{DEMO_CASE} - FOR DAY #{_dday} ...\\n\\n\")\n",
    "    plot_case(DEMO_CASE, day=_dday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.5 MASK SIZES/AREAS</h3>\n",
    "\n",
    "---\n",
    "\n",
    "We know that every other number in an RLE encoding represents a run of mask... so if we add up all those numbers we get the total number of masked pixels in an image. This is much faster than opening and closing each image.\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n",
    "\n",
    "* It's observable that the distributions of mask area is mostly normal although it skews slightly to the smaller side...\n",
    "* All the distributions are similar although the Stomach distribution has an odd gap between 400-750 pixels.\n",
    "* It's interesting to note that, while not common, we do have some VERY large masks (>7500 pixels)\n",
    "    * Also, it's kind of funny that the biggest masks are for **small** bowel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_area(rle):\n",
    "    return sum([int(x) for x in rle.split()[1::2]])\n",
    "\n",
    "train_df[\"lb_seg_area\"] = train_df.lb_seg_rle.apply(lambda x: None if pd.isna(x) else get_mask_area(x))\n",
    "train_df[\"sb_seg_area\"] = train_df.sb_seg_rle.apply(lambda x: None if pd.isna(x) else get_mask_area(x))\n",
    "train_df[\"st_seg_area\"] = train_df.st_seg_rle.apply(lambda x: None if pd.isna(x) else get_mask_area(x))\n",
    "\n",
    "fig = px.histogram(train_df, [\"lb_seg_area\", \"sb_seg_area\", \"st_seg_area\"], title=\"<b>Mask Areas Overlaid</b>\", barmode=\"overlay\",\n",
    "                  labels={\"value\":\"<b>Mask Area</b>\"})\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n\\n\\n... EXAMINE AN EXAMPLE WITH A LARGE AMOUNT OF SEGMENTATION MASK ...\\n\")\n",
    "examine_id(\"case134_day22_slice_0102\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.6 MASK DATASET CREATION, CLASS OVERLAP & MASK HEATMAP</h3>\n",
    "\n",
    "---\n",
    "\n",
    "It's important to determine if the the masks overlap one another (**multilabel**) or not (**multiclass**). To do this, we will quickly create a dataset of **`npy`** files. During this creation process we will check for overlap.\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n",
    "\n",
    "* There is overlap, and while it is not that common, some images exhibit a high degree of overlap.\n",
    "* This means that we cannot frame the problem as simple categorical semantic segmentation.\n",
    "* We must instead frame the problem as multi-label semantic segmentation\n",
    "* This means our mask will take the form --> $W \\times H \\times 3$\n",
    "    * Where the channel dimensions are binary masks for each respective segmentation type\n",
    "    * This will allow for the masks to overlap\n",
    "\n",
    "\n",
    "**NOTE ON THE PLOTTED IMAGE BELOW:**\n",
    "* In the examined image below we can see a section of the small bowel is completely inside of a larger section of larger bowel.\n",
    "* This shows why treating this as multi-label semantic segmentation is so important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_overlap(_arr):\n",
    "    return _arr.sum(axis=-1).max()>1\n",
    "    \n",
    "def make_seg_mask(row, output_dir=\"/data/elastic-notebook/tmp\", check_overlap=False):\n",
    "    slice_shape = (row.slice_w, row.slice_h)\n",
    "    if not pd.isna(row.lb_seg_rle):\n",
    "        lb_mask = rle_decode(row.lb_seg_rle, slice_shape, )\n",
    "    else:\n",
    "        lb_mask = np.zeros(slice_shape)\n",
    "    if not pd.isna(row.sb_seg_rle):\n",
    "        sb_mask = rle_decode(row.sb_seg_rle, slice_shape)\n",
    "    else:\n",
    "        sb_mask = np.zeros(slice_shape)\n",
    "    if not pd.isna(row.st_seg_rle):\n",
    "        st_mask = rle_decode(row.st_seg_rle, slice_shape)\n",
    "    else:\n",
    "        st_mask = np.zeros(slice_shape)\n",
    "    mask_arr = np.stack([lb_mask, sb_mask, st_mask], axis=-1).astype(np.uint8)\n",
    "    np.save(f\"/data/elastic-notebook/tmp/npy_files/{row.id}_mask\", mask_arr)\n",
    "    \n",
    "    if check_overlap: \n",
    "        if is_overlap(mask_arr): \n",
    "            return np.where(mask_arr.sum(axis=-1)>1, 1, 0).sum()\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "NPY_DIR = \"/data/elastic-notebook/tmp/npy_files\"\n",
    "if not os.path.isdir(NPY_DIR): os.makedirs(NPY_DIR, exist_ok=True)\n",
    "train_df[\"seg_overlap_area\"] = train_df.progress_apply(lambda x: make_seg_mask(x, output_dir=NPY_DIR, check_overlap=True), axis=1)\n",
    "\n",
    "print(\"\\n... LET'S EXAMINE THE IMAGE WITH THE HIGHEST AMOUNT OF OVERLAP ...\\n\")\n",
    "\n",
    "examine_id(train_df[train_df.seg_overlap_area==train_df.seg_overlap_area.max()].id.values[0])\n",
    "\n",
    "fig = px.histogram(train_df[train_df.seg_overlap_area>0], \"seg_overlap_area\", color=\"seg_combo_str\", nbins=50,\n",
    "                   log_y=True, title=\"<b>Distribution of Non-Zero Segmentation Overlaps <sub>(Count Is Logarithmic)</sub></b>\",  \n",
    "                   labels={\"seg_overlap_area\":\"<b>Area of Mask Overlap</b>\", \n",
    "                           \"seg_combo_str\":\"<b>Segmentation Masks In Image</b>\"})\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=0.99,\n",
    "    xanchor=\"right\",\n",
    "    x=0.995\n",
    "    ))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = np.zeros((256,256,3), dtype=np.float32)\n",
    "for _, _row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "    if (_row.lb_seg_flag or _row.sb_seg_flag or _row.st_seg_flag):\n",
    "        _mask = cv2.resize(np.load(f\"/data/elastic-notebook/tmp/npy_files/{_row.id}_mask.npy\"), (256,256), interpolation=cv2.INTER_NEAREST)\n",
    "        heatmap+=_mask\n",
    "\n",
    "heatmap=heatmap/heatmap.max()\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(heatmap[..., 0], cmap=\"magma\")\n",
    "plt.title(\"Large Bowel Segmentation Mask ‚Äì Heat Map\", fontweight=\"bold\")\n",
    "plt.axis(False)\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(heatmap[..., 1], cmap=\"magma\")\n",
    "plt.title(\"Small Bowel Segmentation Mask ‚Äì Heat Map\", fontweight=\"bold\")\n",
    "plt.axis(False)\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(heatmap[..., 2], cmap=\"magma\")\n",
    "plt.title(\"Stomach Segmentation Mask ‚Äì Heat Map\", fontweight=\"bold\")\n",
    "plt.axis(False)\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(heatmap)\n",
    "plt.title(\"All Sementation Masks Combined ‚Äì Heat Map\", fontweight=\"bold\")\n",
    "plt.axis(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.7 PIXEL VALUES IN OUR DATASET</h3>\n",
    "\n",
    "---\n",
    "\n",
    "It's important to analyse the dataset because we will need to normalize the data to convert it into a format that is more expected for machine learning (uint8 (0-255) or float32 (0-1)). Without knowing the limits of the images, we may diminish the resolution of the data by accident when normalizing.\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n",
    "\n",
    "Interestingly the maximum value in the dataset is equiavlent to less than half of an int16 or a quarter of a uint16.\n",
    "* Max Value for UINT16\n",
    "    * **65535**\n",
    "* Max Value for INT16\n",
    "    * **32767**\n",
    "* Half of Max Value for INT16\n",
    "    * **16384**\n",
    "* Actual Max Value in the dataset\n",
    "    * **15865**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_vals(row):\n",
    "    _img = cv2.imread(row.f_path, -1)\n",
    "    _nonzero_px_count = np.count_nonzero(_img)\n",
    "    \n",
    "    row[\"nonzero_num_pxs\"] = _nonzero_px_count\n",
    "    row[\"max_px_value\"] = _img.max()\n",
    "    row[\"min_px_value\"] = _img.min()\n",
    "    row[\"mean_px_value\"] = _img.mean()\n",
    "    row[\"nonzero_mean_px_value\"] = _img.sum()/_nonzero_px_count\n",
    "    \n",
    "    return row\n",
    "\n",
    "train_df = train_df.progress_apply(get_image_vals, axis=1)\n",
    "\n",
    "print(f\"\\n\\n\\n... UPDATED TRAIN DATAFRAME ...\\n\")\n",
    "display(train_df.head())\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "for _c in [\"nonzero_num_pxs\", \"max_px_value\", \"min_px_value\", \"mean_px_value\", \"nonzero_mean_px_value\"]:\n",
    "    print(f\"\\n... STATS FOR COLUMN --> `{_c}`...\")\n",
    "    print(f\"\\t--> MIN  VAL: {train_df[_c].min():.1f}\")\n",
    "    print(f\"\\t--> MEAN VAL: {train_df[_c].mean():.1f}\")\n",
    "    print(f\"\\t--> MAX  VAL: {train_df[_c].max():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.8 IDENTIFY ANY HEURISTICS OR RULES REGARDING SEGMENTATION</h3>\n",
    "\n",
    "---\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">SLICEWISE OBSERVATIONS</b>\n",
    "\n",
    "For a given **case-id** and **day number** there are two different amounts of scans present\n",
    "* 144 slices --> 259 instances\n",
    "* 80 slices ---> 15 instances\n",
    "\n",
    "* There are no examples for slices number **1, 138, 139, 140, 141, 142, 143 or 144** that have any segmentation masks\n",
    "* If we break it down by organ we get the following no-value slices for each respective organ\n",
    "    * **Large Bowel** ‚Äì **1, 138, 139, 140, 141, 142, 143, 144**\n",
    "    * **Small Bowel** ‚Äì **1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 138, 139, 140, 141, 142, 143, 144**\n",
    "    * **Stomach** ‚Äì **1, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144**\n",
    "    \n",
    "<br><br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">ROLLING SLICE OBSERVATIONS</b>\n",
    "\n",
    "We next want to identify if there are ever any occurences of a segmentation mask present when there are no segmentation masks before or after (ideally we can measure the prevelance if this exists, and if it doesn't exist, we can identify the minimum number of contiguous segments required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"lb_seg_isolated\"] = False\n",
    "train_df[\"sb_seg_isolated\"] = False\n",
    "train_df[\"st_seg_isolated\"] = False\n",
    "\n",
    "train_df.loc[\n",
    "    (train_df[\"lb_seg_flag\"]==True) &\n",
    "    (train_df[\"lb_seg_flag\"]!=train_df[\"lb_seg_flag\"].shift(1, fill_value=False)) &\n",
    "    (train_df[\"lb_seg_flag\"]!=train_df[\"lb_seg_flag\"].shift(-1, fill_value=False)), \"lb_seg_isolated\"\n",
    "    ] = True\n",
    "\n",
    "train_df.loc[\n",
    "    (train_df[\"sb_seg_flag\"]==True) &\n",
    "    (train_df[\"sb_seg_flag\"]!=train_df[\"sb_seg_flag\"].shift(1, fill_value=False)) &\n",
    "    (train_df[\"sb_seg_flag\"]!=train_df[\"sb_seg_flag\"].shift(-1, fill_value=False)), \"sb_seg_isolated\"\n",
    "    ] = True\n",
    "\n",
    "train_df.loc[\n",
    "    (train_df[\"st_seg_flag\"]==True) &\n",
    "    (train_df[\"st_seg_flag\"]!=train_df[\"st_seg_flag\"].shift(1, fill_value=False)) &\n",
    "    (train_df[\"st_seg_flag\"]!=train_df[\"st_seg_flag\"].shift(-1, fill_value=False)), \"st_seg_isolated\"\n",
    "    ] = True\n",
    "\n",
    "# case43_day18 (lb), case138_day0 (lb), case7_day0 (st)\n",
    "train_df[train_df.lb_seg_isolated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_id(\"case7_day0_slice_0052\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"slice_count\"] = train_df.id.apply(lambda x: int(x.rsplit(\"_\", 1)[-1]))\n",
    "\n",
    "print(\"\\n... CASE-ID/DAY-NUM SLICE INFORMATION ...\\n\")\n",
    "train_df.groupby([\"case_id\", \"day_num\"])[\"slice_count\"].max().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_to_occurence_df = train_df.groupby(\"slice_count\")[[\"lb_seg_flag\", \"sb_seg_flag\", \"st_seg_flag\"]].sum().reset_index()\n",
    "fig = px.bar(slice_to_occurence_df, \n",
    "             x=\"slice_count\", y=[\"lb_seg_flag\", \"sb_seg_flag\", \"st_seg_flag\"],\n",
    "             orientation=\"v\", labels={\n",
    "                 \"slice_count\":\"<b>Slice Number</b>\", \n",
    "                 \"value\":\"<b>Number Of Examples</b>\",\n",
    "             }, title=\"<b>Number of Examples Per Example For Our 3 Organs</b>\")\n",
    "\n",
    "fig.update_layout(\n",
    "    legend_title=\"<b>Organ Type Legend</b>\"\n",
    "    )\n",
    "    \n",
    "fig.show()\n",
    "\n",
    "print(\"\\n... WHICH SLICES ARE ALWAYS BLANK (NO SEG) BY LABEL ...\\n\")\n",
    "keep_slice_blank_map = {_sh_lbl:slice_to_occurence_df[slice_to_occurence_df[f\"{_sh_lbl}_seg_flag\"]==0].slice_count.to_list() for _sh_lbl in [\"lb\", \"sb\", \"st\"]}\n",
    "keep_slice_blank_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.9 CREATE 3D GIF FOR CASE>DAY GROUPS OF SLICES (WITH MASK!!)</h3>\n",
    "\n",
    "---\n",
    "\n",
    "**NOTE: We delete the npy files here because we need to save soon and those will muck things up**\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n",
    "\n",
    "It's beautiful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlay(img_path, rle_strs, img_shape, _alpha=0.999, _beta=0.35, _gamma=0):\n",
    "    _img = open_gray16(img_path, to_rgb=True)\n",
    "    _img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n",
    "    _seg_rgb = np.stack([rle_decode(rle_str, shape=img_shape, color=1) if (rle_str is not None and not pd.isna(rle_str)) else np.zeros(img_shape, dtype=np.float32) for rle_str in rle_strs], axis=-1).astype(np.float32)\n",
    "    seg_overlay = cv2.addWeighted(src1=_img, alpha=_alpha, \n",
    "                                  src2=_seg_rgb, beta=_beta, gamma=_gamma)\n",
    "    return seg_overlay\n",
    "\n",
    "def create_animation(case_id, day_num, df=train_df, save=False, save_dir=\"/data/elastic-notebook/tmp\"):\n",
    "    \n",
    "    sub_df = df[(df.case_id==case_id) & (df.day_num==day_num)]\n",
    "    \n",
    "    f_paths  = sub_df.f_path.tolist()\n",
    "    lb_rles  = sub_df.lb_seg_rle.tolist()\n",
    "    sb_rles  = sub_df.sb_seg_rle.tolist()\n",
    "    st_rles  = sub_df.st_seg_rle.tolist()\n",
    "    slice_ws = sub_df.slice_w.tolist()\n",
    "    slice_hs = sub_df.slice_h.tolist()\n",
    "    \n",
    "    animation_arr = np.stack([\n",
    "        get_overlay(img_path=_f, rle_strs=(_lb, _sb, _st), img_shape=(_w, _h)) \\\n",
    "        for _f, _lb, _sb, _st, _w, _h in \\\n",
    "        zip(f_paths, lb_rles, sb_rles, st_rles, slice_ws, slice_hs)\n",
    "    ], axis=0)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    plt.axis('off')\n",
    "    im = plt.imshow(animation_arr[0])\n",
    "    plt.title(f\"3D Animation for Case {case_id} on Day {day_num}\", fontweight=\"bold\")\n",
    "    \n",
    "    def animate_func(i):\n",
    "        im.set_array(animation_arr[i])\n",
    "        return [im]\n",
    "    plt.close()\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, animate_func, frames = animation_arr.shape[0], interval = 1000//12)\n",
    "    if save:\n",
    "        if not os.path.isdir(save_dir): os.makedirs(save_dir, exist_ok=True)\n",
    "        anim.save(os.path.join(save_dir, f\"case_{case_id}_day_{day_num}.gif\"), fps=10, writer='imagemagick')\n",
    "    return anim\n",
    "\n",
    "create_animation(case_id=115, day_num=0, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As an aside let's create gifs for the 3 case>day pairings that we discovered above have non-contiguous masks**\n",
    "* Some of them definitely have some weird artefacts like the blinking annotation in the last animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case43_day18 (lb)\n",
    "print(\"\\n\\n\\n... LARGE BOWEL ‚Äì NON-CONTIGUOUS EXAMPLE 1 ...\\n\\n\")\n",
    "create_animation(case_id=43, day_num=18, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case138_day0 (lb)\n",
    "print(\"\\n\\n\\n... LARGE BOWEL ‚Äì NON-CONTIGUOUS EXAMPLE 2 ...\\n\\n\")\n",
    "create_animation(case_id=138, day_num=0, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case7_day0 (st)\n",
    "print(\"\\n\\n\\n... STOMACH ‚Äì NON-CONTIGUOUS EXAMPLE 1 ...\\n\")\n",
    "create_animation(case_id=7, day_num=0, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.10 SCANS WITH ERRORS</h3>\n",
    "\n",
    "---\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n",
    "\n",
    "[**Paul G**](https://www.kaggle.com/pgeiger) identified two cases with errors in the segmentation masks in [**this discussion post**](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/discussion/319963)\n",
    "\n",
    "* Case 7 ‚Äì Day 0\n",
    "\n",
    "<img src=\"https://i.ibb.co/M8p8Xfk/case7-day0-slice-0096.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "* Case 81 ‚Äì Day 30\n",
    "\n",
    "<img src=\"https://i.ibb.co/jkdcdzR/case81-day30-slice-0096.png\">\n",
    "\n",
    "---\n",
    "\n",
    "In addition to this, we can see from the above animations that other case/day pairings have errors (or at the very least strong inconsistencies). We plot the remaining error scan below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_case_1 = 7\n",
    "problem_day_1  = 0\n",
    "problem_case_2 = 81\n",
    "problem_day_2  = 30\n",
    "\n",
    "print(\"\\n... PROBLEM CASE NUMBER 1 ...\\n\")\n",
    "plot_case(problem_case_1, day=problem_day_1)\n",
    "\n",
    "print(\"\\n... PROBLEM CASE NUMBER 2 ...\\n\")\n",
    "plot_case(problem_case_2, day=problem_day_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\\n... PLOT REMAINING ERROR SCAN ...\\n\")\n",
    "create_animation(case_id=81, day_num=30, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.11 APPROXIMATE CLASS WEIGHTING (CATEGORICAL ASSUMPTION)</h3>\n",
    "\n",
    "---\n",
    "\n",
    "Let us calculate a naive approximation of the weights of classes based on the frequency of occurence of various classes. For the purpose of this investigation we will treat the background as it's own class (the most common class probably).\n",
    "\n",
    "<br>\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n",
    "\n",
    "* Class **0** - **Background**\n",
    "  * Total Pixel Count In Training Dataset = **0.00**\n",
    "  * Percentage Of All Pixels In Training Dataset = **0.00**\n",
    "  \n",
    "<br>\n",
    "\n",
    "* Class **1** - **Large Bowel**\n",
    "  * Total Pixel Count In Training Dataset = **0.00**\n",
    "  * Percentage Of All Pixels In Training Dataset = **0.00**\n",
    "  \n",
    "<br>\n",
    "  \n",
    "* Class **2** - **Small Bowel**\n",
    "  * Total Pixel Count In Training Dataset = **0.00**\n",
    "  * Percentage Of All Pixels In Training Dataset = **0.00**\n",
    " \n",
    "<br>\n",
    "  \n",
    "* Class **3** - **Stomach**\n",
    "  * Total Pixel Count In Training Dataset = **0.00**\n",
    "  * Percentage Of All Pixels In Training Dataset = **0.00**\n",
    "  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get total image area\n",
    "train_df[\"img_px_area\"] = train_df[\"slice_w\"]*train_df[\"slice_h\"]\n",
    "train_df[[\"lb_seg_area\", \"sb_seg_area\", \"st_seg_area\"]].fillna(0, inplace=True)\n",
    "train_df[\"bg_area\"] = (train_df[\"img_px_area\"] - train_df[[\"lb_seg_area\", \"sb_seg_area\", \"st_seg_area\"]].sum(axis=1)).astype(int)\n",
    "\n",
    "print(f\"\\nALL TRAINING DATA PIXEL COUNT         : {train_df.img_px_area.sum()}\")\n",
    "print(f\"BACKGROUND TRAINING DATA PIXEL COUNT  : {train_df.bg_area.sum()}\")\n",
    "print(f\"LARGE BOWEL TRAINING DATA PIXEL COUNT : {train_df.lb_seg_area.sum()}\")\n",
    "print(f\"SMALL BOWEL TRAINING DATA PIXEL COUNT : {train_df.sb_seg_area.sum()}\")\n",
    "print(f\"STOMACH TRAINING DATA PIXEL COUNT     : {train_df.st_seg_area.sum()}\\n\")\n",
    "\n",
    "print(f\"\\nALL TRAINING DATA PIXEL COUNT (%)         : %{100:.4f}\")\n",
    "print(f\"BACKGROUND TRAINING DATA PIXEL COUNT (%)  : %{100*train_df.bg_area.sum()/train_df.img_px_area.sum():.4f}\")\n",
    "print(f\"LARGE BOWEL TRAINING DATA PIXEL COUNT (%) : %{100*train_df.lb_seg_area.sum()/train_df.img_px_area.sum():.4f}\")\n",
    "print(f\"SMALL BOWEL TRAINING DATA PIXEL COUNT (%) : %{100*train_df.sb_seg_area.sum()/train_df.img_px_area.sum():.4f}\")\n",
    "print(f\"STOMACH TRAINING DATA PIXEL COUNT (%)     : %{100*train_df.st_seg_area.sum()/train_df.img_px_area.sum():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a id=\"modelling\"></a>\n",
    "\n",
    "<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: teal;\" id=\"modelling\">5&nbsp;&nbsp;MODELLING&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">5.1 TEST FORMATTING SUBMISSION</h3>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_px_map = {}\n",
    "for _, row in train_df.groupby([\"slice_h\", \"slice_w\", \"px_spacing_h\", \"px_spacing_w\", \"slice_id\"])[[\"lb_seg_rle\", \"sb_seg_rle\", \"st_seg_rle\"]].first().reset_index().iterrows():\n",
    "    slice_px_map[f\"{row['slice_h']}-{row['slice_w']}-{row['px_spacing_h']}-{row['px_spacing_w']}-{row['slice_id']}-large_bowel\"] = row[\"lb_seg_rle\"]\n",
    "    slice_px_map[f\"{row['slice_h']}-{row['slice_w']}-{row['px_spacing_h']}-{row['px_spacing_w']}-{row['slice_id']}-small_bowel\"] = row[\"sb_seg_rle\"]\n",
    "    slice_px_map[f\"{row['slice_h']}-{row['slice_w']}-{row['px_spacing_h']}-{row['px_spacing_w']}-{row['slice_id']}-stomach\"] = row[\"st_seg_rle\"]\n",
    "\n",
    "ss_df[\"ident\"] = ss_df['slice_h'].astype(str)+\"-\"+ss_df['slice_w'].astype(str)+\"-\"+ss_df['px_spacing_h'].astype(str)+\"-\"+ss_df['px_spacing_w'].astype(str)+\"-\"+ss_df['slice_id'].astype(str)+\"-\"+ss_df[\"class\"].astype(str)\n",
    "ss_df[\"predicted\"] = ss_df[\"ident\"].map(slice_px_map)\n",
    "ss_df = ss_df[[\"id\", \"class\", \"predicted\"]]\n",
    "\n",
    "ss_df.to_csv(\"/data/elastic-notebook/tmp/submission.csv\", index=False)\n",
    "display(ss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">5.2 DICE SCORE EXPLAINED USING SOME OF OUR PREDICTED MASKS</h3>\n",
    "\n",
    "---\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">F1 SCORE</b>\n",
    "\n",
    "<sub><b>REFERENCE --> This <a href=\"https://towardsdatascience.com/essential-things-you-need-to-know-about-f1-score-dbd973bf1a3\">Brilliant Medium Article</a> --> Please Give it Some Claps!</b></sub>\n",
    "\n",
    "<br>\n",
    "\n",
    "By definition, F1-score is the harmonic mean of precision and recall. It combines precision and recall into a single number using the following formula:\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/530/0*1dd6yQxvpZxpdjBX\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "This formula can also be equivalently written as,\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/426/0*JFYDif-juwSY5TFI\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Remember the definition of Precision and Recall are...\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/888/1*7J08ekAwupLBegeUI8muHA.png\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Notice that F1-score takes both precision and recall into account, which also means it accounts for both FPs and FNs. The higher the precision and recall, the higher the F1-score. F1-score ranges between 0 and 1. The closer it is to 1, the better the model.\n",
    "\n",
    "---\n",
    "\n",
    "Now that we‚Äôve covered the fundamentals, let‚Äôs walk through the thinking process of choosing between precision, recall and F1-score. Suppose we have trained three different models for cancer prediction, and each model has different precision and recall values.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*cLzC3-sdOZ5hLc5Y68dEog.gif\"></center>\n",
    "\n",
    "* If we assess that errors caused by FPs (Scenario #2 in Figure 1) are more undesirable, then we will select a model based on precision and choose Model C.\n",
    "* If we assess that errors caused by FNs (Scenario #3 in Figure 1) are more undesirable, then we will select a model based on recall and choose Model B.\n",
    "* However, if we assess that both types of errors are undesirable, then we will select a model based on F1-score and choose Model A.\n",
    "\n",
    "So, the takeaway here is that the model you select depends greatly on the evaluation metric you choose, which in turn depends on the relative impacts of errors of FPs and FNs in your use-case.\n",
    "\n",
    "<br>\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">DICE SCORE</b>\n",
    "\n",
    "<br>\n",
    "\n",
    "The very simple explanation here... is <b>that the Dice Score is simply <font color=\"red\">2*F1-Score</font></b> The other metric we should consider understanding is the Jaccard (IoU) Score. For clarity both those formulas are shown below:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "Dice = \\frac{2 TP}{2TP+FP+FN}, \\qquad Jaccard = IoU =  \\frac{TP}{TP+FP+FN}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**And here is a visualization of the dice score**\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/858/1*yUd5ckecHjWZf6hGrdlwzA.png\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">3D Hausdorff Distance</b>\n",
    "\n",
    "<br>\n",
    "\n",
    "**WIKIPEDIA DEFINITION**\n",
    "\n",
    "In mathematics, the Hausdorff distance, or Hausdorff metric, also called Pompeiu‚ÄìHausdorff distance,[1][2] measures how far two subsets of a metric space are from each other. It turns the set of non-empty compact subsets of a metric space into a metric space in its own right. It is named after Felix Hausdorff and Dimitrie Pompeiu.\n",
    "\n",
    "Informally, two sets are close in the Hausdorff distance if every point of either set is close to some point of the other set. The Hausdorff distance is the longest distance you can be forced to travel by an adversary who chooses a point in one of the two sets, from where you then must travel to the other set. In other words, it is the greatest of all the distances from a point in one set to the closest point in the other set.\n",
    "\n",
    "This distance was first introduced by Hausdorff in his book Grundz√ºge der Mengenlehre, first published in 1914, although a very close relative appeared in the doctoral thesis of Maurice Fr√©chet in 1906, in his study of the space of all continuous curves from $[0,1]\\to \\mathbb {R} ^{3}$\n",
    "\n",
    "<br>\n",
    "\n",
    "**REDDIT ELI5 DEFINITION**\n",
    "\n",
    "Let's say I give you a bunch of objects. For each pair of those objects, I give you a number that we'll call the \"distance\" between them. It doesn't matter whether the number is really the distance between them‚Äîif you move the objects around the number stays the same‚Äîwe're just going to call it that.\n",
    "\n",
    "Now, let's say you pick some of those objects and split them into two groups. We can ask if there's a good way to define the \"distance\" between those two groups. The answer given by Hausdorff, called the Hausdorff distance, is as follows:\n",
    "\n",
    "For each thing in the first set, find the thing in the second set it's closest to and write down that distance. Then, after you've done that for each thing in the first set, take the biggest number you just wrote down.\n",
    "\n",
    "Now switch. For each thing in the second set, find the thing it's closed to in the first set and write down that distance. Then, after you've done that for each thing in the second set, take the biggest number you just wrote down.\n",
    "\n",
    "Now you have two numbers. The bigger of those is the Hausdorff distance.\n",
    "\n",
    "Now, that's all well and good if there's only a finite number of objects involved. If there are an infinite number of objects, then you have to be a little more careful about \"closest\" and \"biggest\" in finding the two numbers above, but it's still basically the same idea.\n",
    "\n",
    "An example might help. Let's take our objects to be the numbers {1, 2, 3, 4, 5, 6}. We'll call the distance between those numbers their difference, so the distance between, say, 2 and 5 is 3. Alright, now let's consider the Hausdorff distance between the groups {1, 2, 3} and {2, 6}.\n",
    "\n",
    "First, we take 1 and find that the smallest distance between it and anything in the second group is 1. Then we take 2 and find that the smallest distance is 0, then we take 3 and we find that the smallest distance is 1. The biggest of these distances is 1.\n",
    "\n",
    "Now we take 2 and find that the smallest distance to an element in the first set is 0, and we do so for 6 and get 3. The biggest of these distances is 3.\n",
    "\n",
    "Taking the bigger of our two numbers, we see that the Hausdorff distance between {1, 2, 3} and {2, 6} is 3.\n",
    "\n",
    "---\n",
    "\n",
    "#### <b><center><font color=\"red\">THIS IS WIP ‚Äì DISREGARD FOR NOW</font></center></b>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial._hausdorff import directed_hausdorff\n",
    "\n",
    "f1score = tfa.metrics.F1Score(num_classes=3)\n",
    "def tf_hausdorff_distance(point_set_a, point_set_b):\n",
    "    difference = point_set_a-point_set_b\n",
    "    # Calculate the square distances between each two points: |ai - bj|^2.\n",
    "    square_distances = tf.einsum(\"...i,...i->...\", difference, difference)\n",
    "    minimum_square_distance_a_to_b = tf.math.reduce_min(square_distances, axis=-1)\n",
    "    return tf.math.sqrt(tf.math.reduce_max(minimum_square_distance_a_to_b, axis=-1))\n",
    "\n",
    "def HausdorffDist(A,B):\n",
    "    # Find pairwise distance\n",
    "    D_mat = np.sqrt(inner1d(A,A)[np.newaxis].T + inner1d(B,B)-2*(np.dot(A,B.T)))\n",
    "    # Find DH\n",
    "    dH = np.max(np.array([np.max(np.min(D_mat,axis=0)),np.max(np.min(D_mat,axis=1))]))\n",
    "    return(dH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_CASE_1 = \"case123_day20\" # Assume this is the ground truth\n",
    "DEMO_CASE_2 = \"case123_day0\" # Assume this is the ground truth\n",
    "\n",
    "lb_seg_arr_1 = np.stack([rle_decode(x, (266,266)) if not pd.isna(x) else np.zeros((266,266), dtype=np.uint8) for x in train_df[train_df.id.str.contains(DEMO_CASE_1)].lb_seg_rle ], axis=0).astype(np.double)\n",
    "sb_seg_arr_1 = np.stack([rle_decode(x, (266,266)) if not pd.isna(x) else np.zeros((266,266), dtype=np.uint8) for x in train_df[train_df.id.str.contains(DEMO_CASE_1)].sb_seg_rle ], axis=0).astype(np.double)\n",
    "st_seg_arr_1 = np.stack([rle_decode(x, (266,266)) if not pd.isna(x) else np.zeros((266,266), dtype=np.uint8) for x in train_df[train_df.id.str.contains(DEMO_CASE_1)].st_seg_rle ], axis=0).astype(np.double)\n",
    "\n",
    "lb_seg_arr_2 = np.stack([rle_decode(x, (266,266)) if not pd.isna(x) else np.zeros((266,266), dtype=np.uint8) for x in train_df[train_df.id.str.contains(DEMO_CASE_2)].lb_seg_rle ], axis=0).astype(np.double)\n",
    "sb_seg_arr_2 = np.stack([rle_decode(x, (266,266)) if not pd.isna(x) else np.zeros((266,266), dtype=np.uint8) for x in train_df[train_df.id.str.contains(DEMO_CASE_2)].sb_seg_rle ], axis=0).astype(np.double)\n",
    "st_seg_arr_2 = np.stack([rle_decode(x, (266,266)) if not pd.isna(x) else np.zeros((266,266), dtype=np.uint8) for x in train_df[train_df.id.str.contains(DEMO_CASE_2)].st_seg_rle ], axis=0).astype(np.double)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
