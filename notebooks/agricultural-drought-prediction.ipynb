{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AGRICULTURAL DROUGHT PREDICTION with 98.925**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 07:07:06.858022: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-25 07:07:06.858046: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#import the necessary libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imshow\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Using the dataset from https://www.kaggle.com/crawford/deepsat-sat4**\n",
    "\n",
    "The input data was encoded into CSV files. The X_test_sat4.csv flattened the images that were 28 x 28 x 4 that were taken from space. The first three channels are the standard red, green, and blue channels in normal images. The 4th is a near-infrared band. We are using the smaller test set because the training set is too big. After extracting the data from the csv files, we can reshape it into the original images. Then, we can see the images before we train on them. The second file we are loading are the labels for each image. They can be one of 4: barren land, trees, grassland and other. Each row in the file looks like this [1,0,0,0], where only one of the 4 value is 1. If it is one, then it is that class respective to the order I showed above. If it was the above values, the image is a picture of barren land. If it was [0,1,0,0], then it would be forest land. If it was [0,0,1,0], then it would be grassland and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Reading dataset\n",
    "X = pd.read_csv(\"/data/elastic-notebook/data/deepsat-sat4/X_test_sat4.csv\") #values are in DataFrame format\n",
    "Y = pd.read_csv(\"/data/elastic-notebook/data/deepsat-sat4/y_test_sat4.csv\") #values are in DataFrame format\n",
    "X = np.ascontiguousarray(X) # converting Dataframe to numpy array\n",
    "Y = np.ascontiguousarray(Y) # converting Dataframe to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (99999, 3136)\n"
     ]
    }
   ],
   "source": [
    "#Shape of data used\n",
    "print(\"Train data shape: \",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped data format:  (99999, 28, 28, 4)\n"
     ]
    }
   ],
   "source": [
    "#reshaping (99999, 3136) to (99999, 28, 28, 4)\n",
    "X = X.reshape([99999,28,28,4]).astype(float)\n",
    "print(\"Reshaped data format: \",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train data shape:  (79999, 28, 28, 4)\n",
      "Y train data shape:  (79999, 4)\n",
      "X test data shape:  (20000, 28, 28, 4)\n",
      "Y test data shape:  (20000, 4)\n"
     ]
    }
   ],
   "source": [
    "#format of train and test data\n",
    "print(\"X train data shape: \",x_train.shape)\n",
    "print(\"Y train data shape: \",y_train.shape)\n",
    "print(\"X test data shape: \",x_test.shape)\n",
    "print(\"Y test data shape: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Four type of classification is possible i.e**\n",
    "* if y_train[x] == [1,0,0,0] Barren land(Drought)\n",
    "* if y_train[x] == [0,1,0,0] Forest land\n",
    "* if y_train[x] == [0,0,1,0] Grassland\n",
    "* if y_train[x] == [0,0,0,1] Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing train and test data\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: Barren Land\n"
     ]
    }
   ],
   "source": [
    "#Images in the data with its label(reduced image)\n",
    "img_no = 1276 #type a random number in inclusive range 0 to 79999\n",
    "imshow(np.squeeze(x_train[img_no,:,:,0:3]).astype(float)) #taking only RGB format\n",
    "plt.show()\n",
    "print ('Ground Truth: ',end='')\n",
    "if y_train[img_no, 0] == 1:\n",
    "    print ('Barren Land')\n",
    "elif y_train[img_no, 1] == 1:\n",
    "    print ('Forest Land')\n",
    "elif y_train[img_no, 2] == 1:\n",
    "    print ('Grassland')\n",
    "else:\n",
    "    print ('Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **Convulutional Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 07:07:29.593672: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-25 07:07:29.593699: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-25 07:07:29.593720: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (createlab-zl20): /proc/driver/nvidia/version does not exist\n",
      "2023-04-25 07:07:29.593892: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#defining layers\n",
    "num_classes = 4\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,4),padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))                  \n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.1))                  \n",
    "model.add(Dense(num_classes, input_shape=(3136,), activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        1184      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               262272    \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 356,324\n",
      "Trainable params: 356,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2477 - accuracy: 0.9063 - val_loss: 0.0969 - val_accuracy: 0.9674\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0982 - accuracy: 0.9650 - val_loss: 0.0743 - val_accuracy: 0.9749\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0837 - accuracy: 0.9708 - val_loss: 0.0690 - val_accuracy: 0.9781\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0692 - accuracy: 0.9763 - val_loss: 0.0872 - val_accuracy: 0.9709\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0645 - accuracy: 0.9768 - val_loss: 0.0469 - val_accuracy: 0.9841\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0594 - accuracy: 0.9791 - val_loss: 0.0491 - val_accuracy: 0.9844\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0552 - accuracy: 0.9811 - val_loss: 0.0635 - val_accuracy: 0.9776\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0473 - accuracy: 0.9837 - val_loss: 0.1306 - val_accuracy: 0.9607\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0451 - accuracy: 0.9840 - val_loss: 0.0469 - val_accuracy: 0.9856\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0396 - accuracy: 0.9864 - val_loss: 0.0515 - val_accuracy: 0.9826\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0381 - accuracy: 0.9866 - val_loss: 0.0662 - val_accuracy: 0.9846\n",
      "Epoch 12/20\n",
      " 997/1000 [============================>.] - ETA: 0s - loss: 0.0349 - accuracy: 0.9882"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#fitting the data into the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zl20/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/home/zl20/venv/lib/python3.8/site-packages/keras/engine/training.py:1420\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1408\u001b[0m       x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1409\u001b[0m       y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1418\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1419\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[0;32m-> 1420\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1432\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1433\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m/home/zl20/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/home/zl20/venv/lib/python3.8/site-packages/keras/engine/training.py:1716\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1715\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 1716\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1717\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1718\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/home/zl20/venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/home/zl20/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/home/zl20/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zl20/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zl20/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/home/zl20/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/home/zl20/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#fitting the data into the model\n",
    "model.fit(x_train,y_train,batch_size=64, epochs=20, verbose=1, validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting model performance\n",
    "preds = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_no = 587#Type a number between 0 and 20000 inclusive\n",
    "imshow(np.squeeze(x_test[img_no,:,:,0:3]).astype(float)) #Only seeing the RGB channels\n",
    "plt.show()\n",
    "#Predicted classification\n",
    "print ('Predicted Label: ',end='')\n",
    "if preds[img_no, 0]*100  >= 80:\n",
    "    print ('Barren Land')\n",
    "elif preds[img_no, 1]*100 >= 80:\n",
    "    print ('Forest Land')\n",
    "elif preds[img_no, 2]*100 >= 80:\n",
    "    print ('Grassland')\n",
    "else:\n",
    "    print ('Other')\n",
    "\n",
    "#Acutal classification\n",
    "print ('Actual label: ',end='')\n",
    "if y_test[img_no, 0] == 1:\n",
    "    print ('Barren Land')\n",
    "elif y_test[img_no, 1] == 1:\n",
    "    print ('Forest Land')\n",
    "elif y_test[img_no, 2] == 1:\n",
    "    print ('Grassland')\n",
    "else:\n",
    "    print ('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model performance evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "print(\"Accuracy score: \",accuracy_score(y_test, np.round_(preds)))\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, np.round_(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of CNN model is: \", accuracy_score(y_test,np.round_(preds))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini project Implementation**  \n",
    "* 1MS17CS025  D.S Rahul https://www.kaggle.com/dsrhul\n",
    "* 1MS17CS053  M Chandan https://www.kaggle.com/chandanvirat18\n",
    "* 1MS17CS056  Mahantesh Shivanand Shivakale https://www.kaggle.com/mahantesh8\n",
    "* 1MS17CS153  Harini K.R\n",
    "under guidance of Dr. Shilpa Chaudhari, Associate Professor at department of CSE, MSRIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
