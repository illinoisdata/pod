{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data : /data/elastic-notebook/data/nih-chest-xrays-data/data\n",
      "CSV file : /data/elastic-notebook/data/nih-chest-xrays-data/preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "mainpath = '/data/elastic-notebook/data/nih-chest-xrays-data/'\n",
    "datadir = os.path.join(mainpath,'data')\n",
    "csvpath = os.path.join(mainpath,'preprocessed_data.csv')\n",
    "\n",
    "print(f'Image Data : {datadir}')\n",
    "print(f'CSV file : {csvpath}')\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **This notebook is using IG for Explainable. And Basic Model is Inception v3** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 21:21:49.454607: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-22 21:21:49.454655: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version : 2.8.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f'TensorFlow Version : {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This feature_map is followed by Orginal Dataset Guide(https://www.kaggle.com/nickuzmenkov/nih-chest-xrays-tfrecords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_map = {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    'No Finding': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Atelectasis': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Consolidation': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Infiltration': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Pneumothorax': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Edema': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Emphysema': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Fibrosis': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Effusion': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Pneumonia': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Pleural_Thickening': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Cardiomegaly': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Nodule': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Mass': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Hernia': tf.io.FixedLenFeature([], tf.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode TFR datafiles and decode image fromg deocded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tfr_decoder(path, shuffle=True):\n",
    "    def image_decoder(data):\n",
    "        example = tf.io.parse_single_example(data, feature_map) \n",
    "        image = example['image']\n",
    "        image = tf.io.decode_image(image, channels=3)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        image = tf.image.resize_with_pad(image, 150, 150)\n",
    "        image.set_shape([150,150,3])\n",
    "        image = image/255.\n",
    "        \n",
    "        print([label for label in sorted(list(example.keys())) if label!='image' and label!='image_id'])\n",
    "        labels = [tf.cast(example[x], tf.float32) for x in sorted(list(example.keys())) if x!='image_id' and x!='image']\n",
    "        \n",
    "        return image, labels\n",
    "    \n",
    "    data_list = [os.path.join(datadir,x) for x in os.listdir(path)]\n",
    "    split = int(len(data_list)*0.8)\n",
    "    train_data, val_data = data_list[:split], data_list[split:]\n",
    "    \n",
    "    trainds = tf.data.TFRecordDataset(train_data)\n",
    "    trainds = trainds.map(image_decoder, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    valds = tf.data.TFRecordDataset(val_data)\n",
    "    valds = valds.map(image_decoder, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        trainds = trainds.shuffle(1024)\n",
    "        \n",
    "    trainds = trainds.batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    valds = valds.batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return trainds, valds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Check Label's order and shape of data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'No Finding', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 21:21:50.563310: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-22 21:21:50.563345: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-22 21:21:50.563371: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (yongjoo-serv-01): /proc/driver/nvidia/version does not exist\n",
      "2023-02-22 21:21:50.563704: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'No Finding', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 15), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "trainds, valds = tfr_decoder(datadir)\n",
    "print(trainds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model from Keras and set pooling='avg' for setting output shape to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inception_v3 (Functional)   (None, 2048)              21802784  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " batch_normalization_94 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " batch_normalization_95 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 15)                495       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,924,719\n",
      "Trainable params: 22,889,007\n",
      "Non-trainable params: 35,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "effic = tf.keras.applications.EfficientNetB2(\n",
    "    include_top=False, weights=None,input_shape=(150,150,3), pooling='avg')\n",
    "\n",
    "incep = tf.keras.applications.InceptionV3(\n",
    "    include_top=False, weights=None, input_shape=(150,150,3), pooling='avg')\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "            incep,\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(15, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='Adam',\n",
    "             metrics=[tf.keras.metrics.AUC(multi_label=True),'binary_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "     57/Unknown - 133s 2s/step - loss: 0.3321 - auc: 0.5298 - binary_accuracy: 0.8772"
     ]
    }
   ],
   "source": [
    "model.fit(trainds, epochs=3)\n",
    "model.evaluate(valds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast = model.predict(valds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check First Data from Valds and get image data from that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_list = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', \n",
    "              'Hernia', 'Infiltration', 'Mass', 'No Finding', 'Nodule', 'Pleural_Thickening', 'Pneumonia', \n",
    "              'Pneumothorax']\n",
    "columns = {}\n",
    "for label in label_list:\n",
    "    columns[label] = []\n",
    "    \n",
    "for element in valds.as_numpy_iterator():\n",
    "    for label in element[1]:\n",
    "        for n, data in enumerate(label):\n",
    "            columns[label_list[n]].append(data)\n",
    "    \n",
    "valdf = pd.DataFrame(columns)\n",
    "valdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Let's check using Atelectasis Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Example = list(valdf.loc[valdf['Atelectasis'] == 1].head(10).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model is not good for predict :(**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_image_data = []\n",
    "for element in valds.as_numpy_iterator():\n",
    "    for img in element[0]:\n",
    "        img = img*255.0\n",
    "        val_image_data.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here, We are going to use IG. For more information about using IG, Please check official guideline(https://www.tensorflow.org/tutorials/interpretability/integrated_gradients)\n",
    "\n",
    "- target_class_idx is Number of label. for example, if you want check which pixels point Atelectasis Area, just set target_class_idx to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def interpolate_images(baseline,image,alphas):\n",
    "    alphas_x = alphas[:, tf.newaxis, tf.newaxis, tf.newaxis]\n",
    "    baseline_x = tf.expand_dims(baseline, axis=0)\n",
    "    input_x = tf.expand_dims(image, axis=0)\n",
    "    delta = input_x - baseline_x\n",
    "    images = baseline_x +  alphas_x * delta\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradients(images, target_class_idx):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images)\n",
    "        logits = model(images)\n",
    "        probs = tf.nn.softmax(logits, axis=-1)[:,target_class_idx]\n",
    "        return tape.gradient(probs, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def integral_approximation(gradients):\n",
    "    # riemann_trapezoidal\n",
    "    grads = (gradients[:-1] + gradients[1:]) / tf.constant(2.0)\n",
    "    integrated_gradients = tf.math.reduce_mean(grads, axis=0)\n",
    "    return integrated_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def integrated_gradients(baseline,image,target_class_idx, m_steps=150, batch_size=64):\n",
    "    # 1. Generate alphas\n",
    "    alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps)\n",
    "\n",
    "    # Accumulate gradients across batches\n",
    "    integrated_gradients = 0.0\n",
    "\n",
    "    # Batch alpha images\n",
    "    ds = tf.data.Dataset.from_tensor_slices(alphas).batch(batch_size)\n",
    "\n",
    "    for batch in ds:\n",
    "\n",
    "        # 2. Generate interpolated images\n",
    "        batch_interpolated_inputs = interpolate_images(baseline=baseline,\n",
    "                                                   image=image,\n",
    "                                                   alphas=batch)\n",
    "\n",
    "        # 3. Compute gradients between model outputs and interpolated inputs\n",
    "        batch_gradients = compute_gradients(images=batch_interpolated_inputs, \n",
    "                                            target_class_idx=target_class_idx)\n",
    "\n",
    "        # 4. Average integral approximation. Summing integrated gradients across batches.\n",
    "        integrated_gradients += integral_approximation(gradients=batch_gradients)\n",
    "\n",
    "    # 5. Scale integrated gradients with respect to input\n",
    "    scaled_integrated_gradients = (image - baseline) * integrated_gradients\n",
    "    return scaled_integrated_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Let check which pixels are important for Model to doing classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_img_attributions(baseline,image,target_class_idx, m_steps=tf.constant(50),cmap=None,\n",
    "                          overlay_alpha=0.4):\n",
    "\n",
    "    attributions = integrated_gradients(baseline=baseline,image=image,target_class_idx=target_class_idx,\n",
    "                                      m_steps=m_steps)\n",
    "\n",
    "    # Sum of the attributions across color channels for visualization.\n",
    "    # The attribution mask shape is a grayscale image with height and width\n",
    "    # equal to the original image.\n",
    "    attribution_mask = tf.reduce_sum(tf.math.abs(attributions), axis=-1)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, squeeze=False, figsize=(8, 8))\n",
    "\n",
    "    axs[0, 0].set_title('Baseline image')\n",
    "    axs[0, 0].imshow(baseline)\n",
    "    axs[0, 0].axis('off')\n",
    "\n",
    "    axs[0, 1].set_title('Original image')\n",
    "    axs[0, 1].imshow(image)\n",
    "    axs[0, 1].axis('off')\n",
    "\n",
    "    axs[1, 0].set_title('Attribution mask')\n",
    "    axs[1, 0].imshow(attribution_mask, cmap=cmap)\n",
    "    axs[1, 0].axis('off')\n",
    "\n",
    "    axs[1, 1].set_title('Overlay')\n",
    "    axs[1, 1].imshow(attribution_mask, cmap=cmap)\n",
    "    axs[1, 1].imshow(image, alpha=overlay_alpha)\n",
    "    axs[1, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline = tf.zeros(shape=(150,150,3))\n",
    "for num in Example:\n",
    "    _ = plot_img_attributions(image=val_image_data[num],baseline=baseline,target_class_idx=0, m_steps=2400,\n",
    "                              cmap=plt.cm.inferno, overlay_alpha=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
